ZT-IX Brainstorming Progress
Date: 2026-02-10

What I looked up
1. PeeringDB OAuth/OIDC docs:
   - https://docs.peeringdb.com/howto/oauth/
   - https://auth.peeringdb.com/register/
2. ZeroTier API/auth docs:
   - https://docs.zerotier.com/api/
   - https://docs.zerotier.com/api-central/
   - https://docs.zerotier.com/api-tokens/
   - https://docs.zerotier.com/sso/

Key findings that affect architecture
1. PeeringDB supports OpenID Connect login and documented OAuth2/OIDC app registration.
2. PeeringDB access tokens can call PeeringDB APIs with `Authorization: Bearer <access_token>`.
3. PeeringDB user permission introspection is available from:
   - `https://auth.peeringdb.com/oauth2/userinfo?scope=peeringdb.manage%20peeringdb.read`
4. ZeroTier Central API is token-based (bearer API token), not a third-party OAuth login provider for this use case.
5. ZeroTier docs also include OIDC SSO, but that is for ZeroTier service/device SSO, not app-to-Central API auth.

How I would build this app
1. Backend first (FastAPI + PostgreSQL)
   - Implement PeeringDB OIDC Authorization Code flow with PKCE.
   - Persist sessions and map PeeringDB user ID to local user record.
   - Exchange code for tokens server-side and store access token encrypted at rest.
2. Identity and policy layer
   - Query PeeringDB API for ASN/network objects allowed for the user.
   - Use PeeringDB userinfo/permissions scope checks for authorization gates.
   - Enforce allowlist rules (for example, accepted ASN ranges or explicit ASN whitelist).
3. ZeroTier orchestration layer
   - Use a service-owned ZeroTier Central API token (stored as secret).
   - For each approved ASN/user request:
     - create or update member authorization in target ZeroTier network,
     - assign managed IP if needed,
     - write provisioning status/audit events.
4. UX flow
   - Login with PeeringDB -> choose ASN -> submit join request -> admin approval (or policy auto-approval) -> show active member state.
   - Add clear status states: `pending`, `approved`, `provisioning`, `active`, `failed`.
5. Operational safety
   - Idempotent provisioning jobs.
   - Retry with exponential backoff for ZeroTier API errors.
   - Full audit log for all auth, approvals, and provisioning changes.

Initial milestone breakdown
1. Milestone 1: Auth and onboarding
   - OIDC login, callback, session handling, ASN fetch.
2. Milestone 2: Membership workflow
   - Join request model, admin approve/reject, status tracking.
3. Milestone 3: ZeroTier provisioning
   - Central API client, member authorization, reconciliation task.
4. Milestone 4: Hardening
   - Tests, observability, rate limits, CSRF/session security.

Open decisions before implementation
1. Should approvals be fully automatic for verified ASN ownership or always admin-gated?
2. Is one shared ZeroTier network sufficient, or does each ASN get isolated network segments?
3. What exact PeeringDB scopes should be considered minimum for production?

---

Update: 2026-02-10

Decision
1. ZeroTier provisioning will use a provider abstraction with two supported modes:
   - `central` (ZeroTier Central API)
   - `self_hosted_controller` (self-hosted ZeroTier controller API via local service auth)
2. Scope remains control-plane independence only; automated roots/planet management is out of scope for phase 1.

Completed
1. Updated planning docs to remove Central-only assumptions and define provider-based provisioning:
   - `PRD.md`
   - `APP_FLOW.md`
   - `TECH_STACK.md`
   - `BACKEND_STRUCTURE.md`
   - `IMPLEMENTATION_PLAN.md`

In progress
1. No implementation code yet. Documentation alignment is complete.

Next
1. Implement Phase 5 Step 5.1 through Step 5.4:
   - provider interface
   - `central` adapter
   - `self_hosted_controller` adapter
   - Celery task provider selection from `ZT_PROVIDER`
2. Add provider contract tests and adapter selection tests.
3. Update `.env.example` for provider mode and credentials.

Known risks / notes
1. Self-hosted controller endpoint/auth behavior must be validated in integration tests before production rollout.
2. Provider API differences can cause behavior drift without strict contract tests.

---

Update: 2026-02-10 (Phase 1 bootstrap complete)

Completed
1. Implemented Phase 1 Step 1.1 project structure bootstrap:
   - Added `app/`, `tests/`, and `alembic/` scaffolding.
   - Added runnable FastAPI app entrypoint with health endpoint.
2. Implemented Phase 1 Step 1.2 pinned dependency setup:
   - Updated `pyproject.toml` with exact runtime and dev versions from `TECH_STACK.md`.
   - Synced environment with `uv sync --dev` and generated `uv.lock`.
3. Implemented Phase 1 Step 1.3 quality tooling configuration:
   - Added `ruff`, `mypy`, and `pytest` configuration in `pyproject.toml`.
   - Added baseline API health test.
4. Implemented Phase 1 Step 1.4 environment bootstrap:
   - Added `.env.example` with required secrets, endpoint settings, and provider mode variables.
5. Added missing `lessons.md` stub as required by `AGENTS.md`.
6. Updated bootstrap run instructions in `README.md`.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 2 Step 2.1: SQLAlchemy model layer per `BACKEND_STRUCTURE.md`.
2. Start Phase 2 Step 2.2: Alembic initial migration from modeled schema.
3. Add Phase 2 database invariant tests (unique active request constraint and status transition rules).

Known bugs / regressions
1. None identified in Phase 1 bootstrap scope.

Notes
1. `uv run` and `uv sync` required elevated permissions in this environment due cache directory restrictions.

---

Update: 2026-02-10 (Local dependency profile decision documented)

Decision
1. Selected for local development dependencies:
   - Docker Compose for PostgreSQL and Redis only.
   - API/worker/test processes run directly with `uv run`.

Completed
1. Documented the selected local dependency profile in:
   - `TECH_STACK.md`
   - `IMPLEMENTATION_PLAN.md`
   - `README.md`

In progress
1. No implementation code changes in this update.

Next
1. Start Phase 2 Step 2.1: SQLAlchemy model layer per `BACKEND_STRUCTURE.md`.
2. Start Phase 2 Step 2.2: Alembic initial migration from modeled schema.
3. Add Phase 2 database invariant tests (unique active request constraint and status transition rules).

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 2 data and migration foundation complete)

Completed
1. Implemented Phase 2 Step 2.1 SQLAlchemy data model layer per `BACKEND_STRUCTURE.md`:
   - Added DB base/metadata conventions and status enums.
   - Added ORM models for `app_user`, `user_asn`, `zt_network`, `join_request`, `zt_membership`, `oauth_state_nonce`, and `audit_event`.
   - Added DB session helpers.
2. Implemented Phase 2 Step 2.2 initial Alembic migration:
   - Wired Alembic to SQLAlchemy metadata.
   - Added initial revision creating enum, tables, indexes, and active-request partial unique index.
3. Implemented Phase 2 Step 2.3 repository layer:
   - Added repositories for users, user ASN mappings, join requests, memberships, and audit events.
   - Added repository error types for duplicate active request and invalid state transition.
4. Implemented Phase 2 Step 2.4 DB invariant tests:
   - Added DB fixtures and tests covering:
     - one active request per (`asn`, `zt_network_id`)
     - allowed/forbidden request status transitions

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/db -q` passed.
4. `uv run pytest -q` passed.
5. Migration lifecycle verified with `uv run env DATABASE_URL=sqlite:////tmp/zt_ix_phase2.db alembic upgrade/downgrade/upgrade`.

In progress
1. No active implementation tasks.

Next
1. Start Phase 3 Step 3.1: `/auth/login` state/nonce/PKCE generation.
2. Start Phase 3 Step 3.2: `/auth/callback` state validation and token exchange path.
3. Add auth integration tests for success/failure callback branches.

Known bugs / regressions
1. `alembic upgrade head` against local PostgreSQL could not be validated in this session because PostgreSQL on `localhost:5432` was unavailable.

---

Update: 2026-02-10 (Phase 2 PostgreSQL verification gap closed)

Completed
1. Added concrete local dependency orchestration for documented infrastructure profile:
   - Added `docker-compose.yml` with pinned images for PostgreSQL `16.6` and Redis `7.4.1`.
   - Set PostgreSQL host mapping default to `5433` (`${POSTGRES_HOST_PORT:-5433}:5432`) to avoid local `5432` conflicts.
2. Aligned local default DB connection settings to host port `5433`:
   - `.env.example`
   - `app/db/session.py`
   - `alembic.ini`
3. Added README run instructions for starting local dependencies and documented the `5433` host-port expectation.
4. Fixed a PostgreSQL migration defect in `alembic/versions/20260210_0001_phase2_data_foundation.py`:
   - Removed redundant explicit enum creation causing duplicate `request_status` type creation during `upgrade`.
5. Completed previously blocked PostgreSQL migration verification using alternate host port override in this environment:
   - Ran upgrade/downgrade/upgrade successfully against running local PostgreSQL.

Verification
1. `docker compose ps` (PostgreSQL and Redis healthy).
2. `uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic upgrade head && uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic downgrade base && uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic upgrade head` passed.
3. `uv run ruff check .` passed.
4. `uv run mypy .` passed.
5. `uv run pytest tests/db -q` passed.
6. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 3 Step 3.1: `/auth/login` state/nonce/PKCE generation.
2. Start Phase 3 Step 3.2: `/auth/callback` state validation and token exchange path.
3. Add auth integration tests for success/failure callback branches.

Known bugs / regressions
1. None identified in Phase 2 scope after PostgreSQL verification.

Notes
1. In this environment, host port `5433` was already allocated; verification used `POSTGRES_HOST_PORT=55433` while keeping repo defaults at `5433`.
2. Added an AGENTS.md non-negotiable to keep escalation-sensitive commands on consistent prefixes (for example, `docker compose`, `uv run`) to reduce repeated approvals.

---

Update: 2026-02-10 (Implementation plan updated for route-server Option A + Option B TODO)

Decision
1. Route-server automation is now planned as Option A in active scope:
   - Worker-driven SSH orchestration to remote Ubuntu/Linux route servers.
   - Explicit generated BIRD peer configuration per ASN on every configured route server.
   - ROA/RPKI validation required in the BIRD policy/configuration path for generated peers.
2. Option B (persisted per-route-server state model) is deferred and tracked as a TODO phase after Phase 8.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version 0.3:
   - Added Option A assumptions/requirements.
   - Expanded Phase 5 steps and exit criteria for route-server sync via SSH.
   - Added explicit per-ASN peer generation requirement.
   - Added ROA/RPKI validation requirement.
   - Added post-Phase-8 TODO section for Option B.

In progress
1. No implementation code changes in this update.

Next
1. Align `PRD.md`, `APP_FLOW.md`, and `BACKEND_STRUCTURE.md` with the new route-server Option A scope before implementation begins.

---

Update: 2026-02-10 (Phase 8 lifecycle ownership plan tightened to MVP scope)

Decision
1. Phase 8 now explicitly targets minimum-viable self-hosted controller lifecycle ownership, not parity with ZeroTier Central.
2. `v0.1.0` assumes single-controller lifecycle ownership; HA topology orchestration is deferred and not a Phase 8 blocker.

Completed
1. Revised `IMPLEMENTATION_PLAN.md` Phase 8 to make implementation actionable and bounded:
   - Added explicit scope boundaries (in-scope vs out-of-scope).
   - Rewrote Step 8.1 to Step 8.5 with concrete readiness, network reconciliation, token reload, backup/restore, and test expectations.
   - Tightened exit criteria and verification commands around fail-closed lifecycle gating.
2. Cleaned plan consistency:
   - Updated snapshot heading to include Phase 7 completion.
   - Replaced stale open-question wording that referenced finishing phase 5 before implementation.

In progress
1. No implementation code changes; documentation refinement only.

Next
1. Align `PRD.md`, `APP_FLOW.md`, and `BACKEND_STRUCTURE.md` wording with the revised Phase 8 MVP/non-parity language where needed.
2. Implement Phase 8 Step 8.1 readiness gate and add initial lifecycle audit events.

Known bugs / regressions
1. None identified from this documentation update.

---

Update: 2026-02-10 (Phase 4 request workflow complete)

Completed
1. Implemented Phase 4 Step 4.1 request APIs with ownership and duplicate protections:
   - Added `POST /api/v1/requests`, `GET /api/v1/requests`, `GET /api/v1/requests/{request_id}`.
   - Enforced ASN ownership via `user_asn` mappings and deterministic duplicate conflict responses.
   - Added target-network validation for active `zt_network` IDs before request creation.
2. Implemented Phase 4 Step 4.2 operator workflow pages:
   - Added `/dashboard` and `/requests/{request_id}` routes.
   - Expanded `/onboarding` payload to include eligible ASNs and active ZeroTier networks.
3. Implemented Phase 4 Step 4.3 admin queue/detail and decision APIs:
   - Added `/admin/requests` and `/admin/requests/{request_id}`.
   - Added `POST /api/v1/admin/requests/{request_id}/approve`.
   - Added `POST /api/v1/admin/requests/{request_id}/reject` with reject-reason validation.
   - Added `POST /api/v1/admin/requests/{request_id}/retry` with strict `failed -> approved` guard.
   - Added role-based session guards for operator/admin access checks.
4. Implemented Phase 4 Step 4.4 audit emission for workflow transitions:
   - Added `request.created` and `request.status.changed` audit writes with actor/status metadata.
5. Implemented Phase 4 Step 4.5 workflow integration tests:
   - Added `tests/workflow/test_request_workflow_integration.py`.
   - Added workflow test fixtures in `tests/workflow/conftest.py`.
   - Covered create/duplicate/ownership errors, operator visibility guards, admin approve/reject/retry transitions, and admin authorization failures.
6. Added supporting repository and session updates:
   - Added `app/repositories/zt_networks.py`.
   - Added admin queue filters in `JoinRequestRepository.list_for_admin`.
   - Updated user upsert behavior to preserve existing admin flags during OAuth login.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/workflow -q` passed.
4. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 5 Step 5.1-5.4 provider abstraction/adapters and worker provider selection.
2. Replace Phase 4 provisioning enqueue placeholder with actual Celery task dispatch in Phase 5.
3. Add provisioning contract tests under `tests/provisioning`.

Known bugs / regressions
1. None identified in current Phase 4 scope.

Notes
1. Integration coverage added for Phase 4 API/page workflow paths using in-memory SQLite + OAuth stubbed login flow.
2. Define environment/config contract for route-server SSH fanout (inventory, auth, command policy).
3. Start Phase 3 auth implementation once scope docs are aligned.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 3 auth integration complete in code + automated tests)

Completed
1. Implemented Phase 3 Step 3.1 `/auth/login`:
   - Added OAuth state/nonce/PKCE generation and persistence (`oauth_state_nonce`).
   - Added redirect construction to PeeringDB authorize endpoint with PKCE challenge and nonce.
2. Implemented Phase 3 Step 3.2 `/auth/callback`:
   - Added one-time state consumption and expiry handling.
   - Added token exchange path and nonce validation via `id_token` nonce claim.
   - Added explicit callback failure codes and audit events.
3. Implemented Phase 3 Step 3.3 user + ASN sync from PeeringDB profile:
   - Added PeeringDB client abstraction for token/profile calls and profile normalization.
   - Upserts local `app_user` and replaces `user_asn` mappings based on authorized networks.
4. Implemented Phase 3 Step 3.4 session + logout behavior:
   - Added signed cookie session middleware configuration.
   - Added `/auth/logout` session clear + audit event.
   - Added placeholder guarded `/onboarding` and `/error` routes for auth redirects.
5. Implemented Phase 3 Step 3.5 integration-style auth tests (`tests/auth`):
   - Success callback path.
   - Invalid/missing state path.
   - Token exchange failure path.
   - Nonce mismatch path.
   - Callback replay protection path.
6. Added manual browser integration instructions for Phase 3 in `README.md` (required due no browser in this environment).
7. Expanded `.env.example` with auth/session runtime settings (`APP_ENV`, session cookie flags, OAuth TTL/scopes/timeout).

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/auth -q` passed.
4. `uv run pytest -q` passed.

In progress
1. Phase 3 implementation complete.
2. Real browser-based OIDC checks still pending execution outside this environment (instructions documented in `README.md`).

Next
1. Execute the README browser checklist against a real PeeringDB OAuth app and record outcomes.
2. Start Phase 4 Step 4.1 request creation endpoint with ASN ownership checks.
3. Add API auth helpers for current session user to support Phase 4 route guards.

Known bugs / regressions
1. No automated regressions detected in current local test suite.
2. Nonce verification currently depends on `id_token` presence in token response; browser validation against live PeeringDB remains required.

Notes
1. Added test package `__init__.py` files to keep strict `mypy` module resolution stable with multiple `conftest.py` modules.

---

Update: 2026-02-10 (Phase 3 auth nonce fix + detailed callback error surface)

Decision
1. Enforced OIDC scope requirement by always including `openid` in configured PeeringDB scopes to ensure `id_token` nonce validation can succeed.
2. Expanded callback error redirect contract to include user-visible `detail` codes/messages for faster diagnosis without reading server logs.

Completed
1. Fixed likely `invalid_nonce` root cause path:
   - Updated scope normalization in `app/config.py` to inject `openid` when missing and deduplicate configured scopes.
   - Updated `.env.example` default `PEERINGDB_SCOPES` to `openid profile email networks`.
   - Updated `TECH_STACK.md` documented scope list to include `openid`.
2. Added richer callback error details:
   - Extended `/auth/callback` redirect helper to include `detail` query parameter.
   - Added nonce error detail-code mapping (for example `missing_id_token`, `missing_nonce_claim`, `nonce_mismatch`).
   - Added structured detail propagation for `oauth_error`, state errors, and upstream auth failures.
3. Expanded `/error` response payload:
   - Added user-facing `message` per error code.
   - Added passthrough `detail` field for callback diagnostics.
4. Updated tests:
   - Scope assertion now expects `openid profile email networks`.
   - Added coverage for nonce mismatch detail code.
   - Added coverage for missing `id_token` returning `invalid_nonce` with `missing_id_token` detail and message payload.
   - Added config tests for environment-scope normalization (`openid` injection and deduplication).
5. Updated README Phase 3 instructions to explicitly require `PEERINGDB_SCOPES` including `openid`.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest -q` passed.

In progress
1. No active implementation tasks in this update.

Next
1. Re-run browser integration login against live PeeringDB with updated `PEERINGDB_SCOPES` and verify callback reaches `/onboarding`.
2. Capture any remaining live-provider callback detail codes from `/error` for follow-up hardening if needed.

Known bugs / regressions
1. No regressions detected in local automated checks for this scope.

---

Update: 2026-02-10 (PeeringDB OAuth registration algorithm lesson documented)

Decision
1. Captured a live-integration lesson that successful PeeringDB OIDC callback in this project also depends on OAuth app registration algorithm set to `RSA with SHA-2 256` (RS256).

Completed
1. Updated Phase 3 manual integration guidance in `README.md`:
   - Added explicit OAuth registration prerequisite to set algorithm `RSA with SHA-2 256` (RS256).
2. Updated `TECH_STACK.md` external API constraints:
   - Added PeeringDB registration algorithm requirement as an integration contract note.
3. Replaced `lessons.md` TODO stub with a concrete lesson entry:
   - Symptom, root cause, prevention rule, and good-vs-bad example for the nonce failure scenario.

In progress
1. No active implementation tasks.

Next
1. Keep this registration prerequisite in onboarding/checklist docs for any future environment setup.
2. If additional live-auth pitfalls appear, append them to `lessons.md` and Phase 3 README checklist.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Auth Option A documentation alignment)

Decision
1. Added authentication planning/documentation for Auth Option A:
   - Local credentials table with canonical `app_user` model.
   - Server CLI account provisioning with admin and associated ASN/network options.
2. Clarified option naming to avoid ambiguity with existing route-server options:
   - `Auth Option A` / `Auth Option B`
   - `Route Server Option A` / `Route Server Option B`

Completed
1. Updated `PRD.md` (Version 0.3):
   - Added Auth Option A scope, user stories, acceptance criteria, security expectations, and DoD coverage.
2. Updated `APP_FLOW.md` (Version 0.3):
   - Added `/auth/local/login` flow and explicit server CLI local-account provisioning flow.
3. Updated `BACKEND_STRUCTURE.md` (Version 0.3):
   - Added canonical-user + local-credential data model (`local_credential`, `user_network_access`) and related auth/env/edge-case contracts.
4. Updated `IMPLEMENTATION_PLAN.md` (Version 0.4):
   - Added detailed Auth Option A implementation steps (schema, auth route, CLI provisioning, tests).
   - Disambiguated auth option naming from route-server options.
5. Updated `TECH_STACK.md` (Version 0.3):
   - Documented local credential handling approach and authentication mode support without adding new external dependencies.

In progress
1. No implementation code changes in this update.

Next
1. Resolve open question: for Auth Option A, should empty associated-network assignment be unrestricted or deny-by-default?
2. Start implementation from `IMPLEMENTATION_PLAN.md` Phase 3 Auth Option A steps:
   - schema migration
   - local credential service/route
   - CLI provisioning command
   - tests

Known bugs / regressions
1. None identified for this documentation-only update.

Notes
1. This update changes planning contracts only; runtime behavior is unchanged until implementation starts.

---

Update: 2026-02-10 (Auth Option A implementation completed)

Decision
1. Implemented associated-network authorization semantics as documented in planning:
   - If a user has one or more `user_network_access` rows, request target network must be in that set.
   - If a user has no `user_network_access` rows, network eligibility is unrestricted.

Completed
1. Implemented Phase 3 Auth Option A schema/model alignment:
   - Added ORM models and relationships for `local_credential` and `user_network_access`.
   - Made `app_user.peeringdb_user_id` nullable while preserving uniqueness when present.
   - Added Alembic revision `20260210_0002_auth_option_a_schema.py`.
2. Implemented local credential auth service and login route:
   - Added stdlib PBKDF2-HMAC password hash/verify helpers with constant-time comparison in `app/auth/local_credentials.py`.
   - Added `POST /auth/local/login` with deterministic invalid-credential behavior, disabled-credential handling, session establishment, and audit events.
3. Implemented server CLI local account provisioning:
   - Added `app/cli/users.py` with `create` command supporting:
     - username/password input mode (`--password` or `--password-stdin`)
     - optional admin flags (`--admin` / `--no-admin`)
     - repeatable ASN assignment (`--asn`)
     - repeatable associated-network assignment (`--zt-network-id`)
   - Added provisioning audit event `auth.local_account.provisioned`.
4. Implemented associated-network enforcement in request/onboarding paths:
   - Enforced network restriction checks in `POST /api/v1/requests`.
   - Filtered `/onboarding` network list by `user_network_access` when restrictions exist.
5. Added/updated tests:
   - Added `tests/auth_local` integration coverage for local login success/failure/disabled/toggle/no-ASN and onboarding network filtering.
   - Added `tests/cli` coverage for CLI provisioning success, unknown-network rollback, and password-policy validation.
   - Added workflow integration coverage for associated-network authorization enforcement.
   - Updated config and fixture coverage for new local-auth settings fields.
6. Updated `.env.example` with local-auth runtime settings:
   - `LOCAL_AUTH_ENABLED`
   - `LOCAL_AUTH_PASSWORD_MIN_LENGTH`
   - `LOCAL_AUTH_PBKDF2_ITERATIONS`

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest -q` passed.
4. `uv run env DATABASE_URL=sqlite:////tmp/zt_ix_auth_option_a.db alembic upgrade head && uv run env DATABASE_URL=sqlite:////tmp/zt_ix_auth_option_a.db alembic downgrade base && uv run env DATABASE_URL=sqlite:////tmp/zt_ix_auth_option_a.db alembic upgrade head` passed.

In progress
1. No active implementation tasks in this update.

Next
1. If desired, add operator/admin UI form surfaces for local login and local-account lifecycle actions.
2. If desired, add throttling/lockout controls for repeated local login failures (Phase 7 security hardening).

Known bugs / regressions
1. No regressions detected in local lint/type/tests for this scope.

---

Update: 2026-02-10 (Phase 5 split into self-contained sub-phases)

Decision
1. Phase 5 is now split into smaller documentation sub-phases while preserving original scope and step IDs (`5.1` through `5.14`).

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version 0.5:
   - Added Sub-phase 5A: Provider Foundation and Membership Provisioning.
   - Added Sub-phase 5B: Route Server Desired Config Generation.
   - Added Sub-phase 5C: Route Server Apply and Convergence.
   - Added self-contained outcomes for each sub-phase.
   - Kept consolidated Phase 5 exit criteria and verification section.

In progress
1. No implementation code changes in this update.

Next
1. Execute Sub-phase 5A steps (`5.1` to `5.8`) as the next implementation block.
2. Keep route-server rendering (`5.9` to `5.11`) and apply/convergence (`5.12` to `5.14`) as separate follow-on blocks.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Checklist + blocker tracking format applied to implementation plan)

Decision
1. `IMPLEMENTATION_PLAN.md` is now checklist-driven (`[x]` complete, `[ ]` open) with explicit blocker annotations.
2. Queue dispatch from admin approve/retry is explicitly deferred to Phase 5 while retaining the current placeholder hook.
3. JSON responses for workflow "page" routes are accepted for the current state; UI/template integration is tracked as open and blocked until Phase 6 execution.

Completed
1. Rewrote `IMPLEMENTATION_PLAN.md` to Version 0.6 with:
   - status checklists for all phases and verification items,
   - explicit gap tracking through and beyond Phase 4,
   - concrete blocker metadata (`Blocked by` + `Reason`) for queueing, frontend/UI integration, and route-server dependencies.
2. Updated `AGENTS.md` to enforce:
   - checklist-based implementation plan maintenance,
   - explicit blocker documentation,
   - required backfill of blocked gaps once dependencies are completed.

In progress
1. No implementation code changes in this update.

Next
1. Start Phase 5 Step 5.1 to Step 5.4:
   - provider interface,
   - `central` and `self_hosted_controller` adapters,
   - Celery provider resolution and real queue dispatch replacing the placeholder hook.
2. After Phase 5 stabilizes request/provisioning behavior, execute Phase 6 UI/template integration to replace JSON-only route responses for workflow pages.

Known bugs / regressions
1. No runtime regressions identified; this update is documentation-only.

---

Update: 2026-02-10 (Implementation plan marks auto-approval as configurable)

Decision
1. Auto-approval is now tracked in planning as a configurable approval mode while keeping manual admin approval as the default.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version 0.7:
   - Marked policy auto-approval as a configurable option in planning assumptions.
   - Added explicit Phase 4 checklist item (Step 4.7) for configurable approval-mode support.
   - Added blocker metadata tying auto-approval execution to real async dispatch wiring in Phase 5.
   - Added matching exit-criteria and verification checklist entries for approval-mode behavior and audit coverage.

In progress
1. No implementation code changes in this update.

Next
1. Implement Phase 5 Step 5.1 to Step 5.4 (provider wiring + real Celery dispatch), then execute Phase 4 Step 4.7 for configurable approval-mode behavior.
2. Finalize product/security guardrails for auto-approval eligibility and fallback behavior before implementation.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Sub-phase 5A implemented in code)

Completed
1. Implemented Phase 5 Sub-phase 5A Step 5.1 provider contract foundation:
   - Added provider-agnostic `ProvisionResult` model and provisioning provider protocol.
   - Added normalized provider error taxonomy for auth/network/request failures.
2. Implemented Step 5.2 and Step 5.3 provider adapters:
   - Added ZeroTier Central adapter with token auth (`Authorization: token <token>`).
   - Added self-hosted controller adapter with `X-ZT1-Auth` auth header.
   - Added response normalization for member ID, authorization state, and assigned IPs.
3. Implemented Step 5.4 Celery dispatch and provider-selection wiring:
   - Added Celery task module and real queue dispatch from admin approve/retry paths.
   - Added provider factory selection by `ZT_PROVIDER` with deterministic misconfiguration errors.
4. Implemented Step 5.5 and Step 5.7 provisioning state machine and retry-safe behavior:
   - Added provisioning service to transition `approved -> provisioning -> active|failed`.
   - Added idempotent membership upsert behavior and deterministic failure persistence (`last_error`, retry increment).
   - Added audit events for provisioning start/success/failure/skip branches.
5. Implemented Step 5.6 request API membership visibility coverage:
   - Confirmed request serialization includes membership payload when present.
   - Added integration test coverage for membership inclusion in request detail API responses.
6. Implemented Step 5.8 test coverage:
   - Added provider contract tests for both adapters.
   - Added provider factory selection and credential-validation tests.
   - Added provisioning service tests for success, failure, skip, and idempotent upsert behavior.
   - Added Celery task/dispatch behavior tests and provider-resolution path coverage.
7. Updated implementation tracking docs:
   - Updated `IMPLEMENTATION_PLAN.md` to Version 0.8 with Sub-phase 5A checklist completion and blocker/status updates.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/provisioning tests/workflow -q` passed.
4. `uv run pytest -q` passed.

In progress
1. No active implementation tasks in this update.

Next
1. Implement Phase 5 Sub-phase 5B (Step 5.9 to Step 5.11): route-server sync service, deterministic BIRD peer rendering, and ROA/RPKI policy enforcement.
2. Implement Phase 4 Step 4.7 configurable approval mode once policy guardrails are finalized.

Known bugs / regressions
1. No regressions detected in local lint/type/tests after Sub-phase 5A changes.

---

Update: 2026-02-10 (Sub-phase 5B implemented in code)

Completed
1. Implemented Phase 5 Sub-phase 5B Step 5.9 route-server SSH fanout service:
   - Added `app/provisioning/route_servers.py` for config-driven remote route-server sync.
   - Added runtime settings/env parsing for route-server host inventory, SSH options, remote config dir, and local route-server ASN.
2. Implemented Step 5.10 deterministic per-ASN BIRD snippet rendering:
   - Added deterministic renderer based on request ID, ASN, node ID, and ZeroTier-assigned endpoint IPs.
   - Added one explicit BGP peer stanza per assigned endpoint address.
3. Implemented Step 5.11 ROA/RPKI policy enforcement in generated config path:
   - Added generated BIRD import filters using `roa_check(ztix_roa_v4, ...)` and `roa_check(ztix_roa_v6, ...)`.
4. Integrated route-server sync into provisioning workflow:
   - Route-server sync now runs after successful member authorization in provisioning path.
   - Sync results are audited; sync failures transition request to `failed` with actionable error code/message.
5. Added automated tests for route-server sub-phase behavior:
   - New `tests/route_servers/test_sync.py` covers rendering output, SSH fanout, and aggregated failure handling.
   - Extended provisioning service tests to cover route-server sync success and failure branches.
6. Updated docs and tracking:
   - Updated `.env.example` with route-server settings.
   - Added README route-server setup section (per-router package install, SSH prep, BIRD include, RPKI cron example, syntax check).
   - Aligned product/system docs with route-server Option A scope:
     - `PRD.md`
     - `APP_FLOW.md`
     - `TECH_STACK.md`
     - `BACKEND_STRUCTURE.md`
   - Updated `IMPLEMENTATION_PLAN.md` to Version 0.9 with Step 5.9 to Step 5.11 marked complete.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/provisioning -q` passed.
4. `uv run pytest tests/route_servers -q` passed.
5. Generated-config syntax check passed with `bird -p -c /tmp/ztix-bird.conf`.

In progress
1. No active implementation tasks in this update.

Next
1. Implement Phase 5 Sub-phase 5C (Step 5.12 to Step 5.14): safe BIRD apply/check workflow per route server and convergence gating for request status transitions.
2. Keep configurable approval mode (Phase 4 Step 4.7) blocked pending product/security guardrail sign-off.

Known bugs / regressions
1. None identified in local lint/type/tests for Sub-phase 5B scope.

---

Update: 2026-02-10 (Config split cleanup: `.env.example` vs YAML example)

Decision
1. Split example configuration by concern:
   - `.env.example` now focuses on secrets/runtime wiring values.
   - `runtime-config.example.yaml` now contains non-secret runtime/policy-style settings that were previously listed in `.env.example`.

Completed
1. Cleaned up `.env.example`:
   - Removed non-secret policy/default settings moved to YAML example.
   - Removed unused `PEERINGDB_API_BASE_URL` placeholder that was not consumed in code.
2. Added `runtime-config.example.yaml`:
   - Added example values for session defaults, OAuth/profile endpoints, local-auth tuning, provider base URLs, and route-server SSH/config settings.
3. Updated `README.md`:
   - Documented config split (`.env.example` vs `runtime-config.example.yaml`).
   - Updated route-server setup section to show YAML profile-style config instead of `.env` block.

In progress
1. No active implementation tasks.

Next
1. If desired, implement runtime YAML ingestion and env-override precedence in `app/config.py` so the YAML profile can be consumed directly by the app.

Known bugs / regressions
1. None identified in this docs/config-template cleanup scope.

Notes
1. `docker-compose.yml` did not require changes because it only defines PostgreSQL/Redis service containers and does not consume the moved app-level settings.

---

Update: 2026-02-10 (Owned self-hosted controller lifecycle required by plan/docs)

Decision
1. Repository scope is now documented to own self-hosted ZeroTier controller lifecycle for release environments.
2. `ZT_PROVIDER=self_hosted_controller` is now the documented release expectation; `central` is retained as compatibility-only for migration/testing and is not a release gate.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version 1.0:
   - Added Sub-phase 5D for owned controller lifecycle implementation.
   - Added checklist steps for bootstrap/readiness, network reconciliation, token lifecycle, backup/restore, and lifecycle integration tests.
   - Added self-hosted-only release gates in Phase 5 and Phase 8.
2. Updated product and architecture docs for scope alignment:
   - `PRD.md` to Version 0.5 with new lifecycle ownership feature/acceptance criteria.
   - `APP_FLOW.md` to Version 0.5 with lifecycle preflight/operations flow and provisioning gate semantics.
   - `BACKEND_STRUCTURE.md` to Version 0.5 with lifecycle ownership backend contract and planned lifecycle env variables.
   - `TECH_STACK.md` to Version 0.5 with self-hosted release path and central compatibility-only constraints.
3. Updated operator/runtime documentation:
   - `README.md` to clarify self-hosted lifecycle ownership scope and release expectation.
   - `.env.example` defaulted to `ZT_PROVIDER=self_hosted_controller` and expanded controller runtime values.
   - `runtime-config.example.yaml` updated with self-hosted provider default and lifecycle policy placeholders.

In progress
1. No implementation code changes in this update; this is a source-of-truth documentation alignment pass.

Next
1. Implement `IMPLEMENTATION_PLAN.md` Sub-phase 5C (`5.12` to `5.14`) for route-server apply/convergence.
2. Implement Sub-phase 5D (`5.15` to `5.19`) for owned self-hosted controller lifecycle controls.
3. Add lifecycle test suite (`tests/controller_lifecycle`) and wire release readiness checks for self-hosted-only staging and DR drill sign-off.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 5 sub-phases promoted to standalone phases)

Decision
1. Reorganized `IMPLEMENTATION_PLAN.md` so each former Phase 5 sub-phase is now a standalone phase to reduce section nesting and improve checklist readability.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version 1.1:
   - Split former Phase 5 sub-phases into standalone phases:
     - Phase 5: Provider Foundation and Membership Provisioning
     - Phase 6: Route Server Desired Config Generation
     - Phase 7: Route Server Apply and Convergence
     - Phase 8: Self-Hosted Controller Lifecycle Ownership
   - Renumbered later phases to keep linear progression:
     - Frontend Hardening -> Phase 9
     - Security and Observability -> Phase 10
     - Release Readiness -> Phase 11
     - Route Server Option B TODO -> Phase 12
   - Renumbered affected step IDs and blocker references to match new phase layout.
   - Updated status snapshot and traceability references for the new phase structure.

In progress
1. No implementation code changes; documentation reorganization is complete.

Next
1. Execute Phase 7 Step 7.1 to Step 7.3 for route-server apply/convergence.
2. Execute Phase 8 Step 8.1 to Step 8.5 for self-hosted controller lifecycle ownership.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 7 route-server apply + convergence completed, live check rerun successful)

Completed
1. Implemented Phase 7 Step 7.1 safe route-server apply workflow in `app/provisioning/route_servers.py`:
   - Added per-host apply orchestration after config sync:
     - `bird -p -c /etc/bird/bird.conf`
     - `birdc configure check`
     - timed `birdc configure` (`timeout 20s`)
     - confirmation check (`birdc show status`)
   - Added per-host failure-stage capture and best-effort rollback workflow using backup/restore of managed snippet file.
2. Implemented Phase 7 Step 7.2 convergence gating and actionable failure context in `app/provisioning/service.py`:
   - Request transitions to `active` only when all configured route servers complete sync/apply successfully.
   - Added explicit route-server failure audit event (`route_server.sync.failed`) with structured per-host failure metadata and successful-host context.
3. Implemented Phase 7 Step 7.3 test coverage:
   - Extended `tests/route_servers/test_sync.py` for safe-apply command orchestration and partial-failure + rollback assertions.
   - Extended `tests/provisioning/test_service.py` for route-server failure metadata assertions and retry idempotency after route-server failure.
4. Updated implementation tracking checklist in `IMPLEMENTATION_PLAN.md`:
   - Marked Phase 7 Step 7.1 to Step 7.3 complete.
   - Marked Phase 7 exit criteria complete.
   - Updated Phase 7 verification checklist with automated and manual status.

Verification
1. `uv run pytest tests/route_servers tests/provisioning/test_service.py -q` passed.
2. `uv run ruff check .` passed.
3. `uv run mypy .` passed.
4. `uv run pytest -q` passed.
5. Live integration check rerun against route server after access fixes:
   - Host: `rs1.zsss.org`
   - User: `ztixsync`
   - Sequence passed: `bird -p -c /etc/bird/bird.conf` -> `birdc configure check` -> `timeout 20s birdc configure` -> `birdc show status`

In progress
1. No active implementation task in this update.

Next
1. If required, execute manual failure-injection runbook step for Phase 7 verification item:
   - confirm live failed apply surfaces actionable error context and retry idempotency in deployed environment.
2. Start Phase 8 Step 8.1 to Step 8.5 (self-hosted controller lifecycle ownership).

Known bugs / regressions
1. None detected in automated checks for this scope.

Notes
1. Integration checks required runtime host key material and SSH key-read access alignment in this environment before full remote validation succeeded.

---

Update: 2026-02-10 (Live integration test now creates a temporary BGP session)

Completed
1. Added live route-server integration test `tests/route_servers/test_live_integration.py` that:
   - reads route-server SSH/runtime values from `runtime-config.yaml` (or `ZTIX_RUNTIME_CONFIG_PATH`),
   - pushes a temporary managed snippet via `RouteServerSyncService`,
   - runs safe apply path (`bird -p`, `birdc configure check`, timed `birdc configure`),
   - validates the generated BGP protocol appears in `birdc show protocols all <name>`,
   - removes the temporary snippet and reapplies config in cleanup.
2. Added helper coverage in the same file for protocol-name extraction used by the live assertion path.
3. Updated docs:
   - `README.md` route-server section with an explicit live integration test command and required env vars.
   - `IMPLEMENTATION_PLAN.md` Phase 7 verification checklist with the live integration test command.
4. Executed live integration test against configured host (`rs1.zsss.org`) with runtime SSH credentials and temporary key-path override.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/route_servers tests/provisioning/test_service.py -q` passed.
4. `uv run pytest -q` passed.
5. `ZTIX_RUN_ROUTE_SERVER_INTEGRATION=1 ZTIX_ROUTE_SERVER_TEST_SSH_KEY_PATH=/tmp/ztix_route_server_ed25519 UV_CACHE_DIR=/tmp/uv-cache uv run pytest tests/route_servers/test_live_integration.py -q -k live_route_server_creates_test_bgp_session` passed.

In progress
1. No active implementation task in this update.

Next
1. Execute the remaining Phase 7 manual failure-path check (inject apply failure and verify actionable error + retry idempotency).
2. Start Phase 8 (self-hosted controller lifecycle ownership).

Known bugs / regressions
1. None detected in local or live-integration checks for this scope.

---

Update: 2026-02-10 (Phase 8 plan expanded for containerized ZeroTier controller runtime + validation)

Decision
1. Phase 8 now explicitly includes a prerequisite container-runtime step for `zerotier/zerotier:1.14.2` on port `9993` so lifecycle ownership work is validated in the same compose stack as PostgreSQL and Redis.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version `1.3`:
   - Added Step `8.0` for containerized controller runtime wiring.
   - Captured required runtime details for compose-based operation:
     - `zerotier/zerotier:1.14.2` service definition target,
     - persistent `/var/lib/zerotier-one` volume,
     - `127.0.0.1:9993/tcp` API and `9993/udp` transport exposure,
     - token bootstrap path (`/var/lib/zerotier-one/authtoken.secret`) for `ZT_CONTROLLER_AUTH_TOKEN`.
   - Updated Phase 8 dependency chain so lifecycle implementation steps are blocked by runtime bootstrap where appropriate.
   - Added explicit Phase 8 verification commands for compose startup and authenticated controller probes (`/status`, `/controller`, `/controller/network`).

In progress
1. No implementation code changes in this update; this is a Phase 8 execution-plan refinement.

Next
1. Implement Phase 8 Step `8.0` in infrastructure/docs (`docker-compose.yml`, `.env.example`, `README.md`) so controller runtime bootstrap is executable.
2. Implement Phase 8 Step `8.1` readiness gate behavior in API/worker provisioning paths.
3. Add lifecycle tests and manual drill checklist items in Phase 8 Step `8.5` once runtime + readiness implementation exists.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 8 Step 8.0 implemented with pinned controller tag + token consistency validation)

Completed
1. Implemented Phase 8 Step 8.0 container runtime wiring:
   - Added `zerotier-controller` service to `docker-compose.yml` using `zerotier/zerotier:1.14.2`.
   - Added persistent named volume `zt_ix_zerotier_controller_data` mounted at `/var/lib/zerotier-one`.
   - Exposed controller API on `127.0.0.1:9993/tcp` and controller transport on `9993/udp`.
   - Added required runtime capabilities/devices for controller operation (`/dev/net/tun`, `NET_ADMIN`, `SYS_ADMIN`).
2. Updated docs/config references for pinned image and token bootstrap:
   - `.env.example`
   - `README.md`
   - `TECH_STACK.md`
   - `IMPLEMENTATION_PLAN.md`
3. Validated controller token consistency:
   - `authtoken.secret` stable across repeated reads.
   - Token hash matches across host-exec and in-container reads.
   - Authenticated in-container `/status` probe succeeds with HTTP 200.

Verification
1. `POSTGRES_HOST_PORT=55433 docker compose up -d postgres redis zerotier-controller` passed.
2. `POSTGRES_HOST_PORT=55433 docker compose ps postgres redis zerotier-controller` shows all three services healthy/running.
3. `POSTGRES_HOST_PORT=55433 docker compose exec -T zerotier-controller sh -lc 'TOKEN="$(cat /var/lib/zerotier-one/authtoken.secret)"; curl -fsS -H "X-ZT1-Auth: ${TOKEN}" http://127.0.0.1:9993/status'` passed (HTTP 200 + JSON status payload).
4. Host-side `/status` probe with the same token currently returns `401` in this environment.

In progress
1. Phase 8 Step 8.1 to Step 8.5 not implemented yet.

Next
1. Implement Phase 8 Step 8.1 readiness gate in API/worker provisioning path.
2. Implement Phase 8 Step 8.2 network reconciliation.
3. Implement Phase 8 Step 8.3 token lifecycle controls.

Known bugs / regressions
1. Host-side `curl http://127.0.0.1:9993/status` currently returns `401` in this compose bridge topology even when token material is consistent; in-container probe is reliable.

---

Update: 2026-02-10 (Phase 8 Step 8.0 host-side management reachability fixed)

Decision
1. Fixed host-side controller API `401` by explicitly configuring ZeroTier management allowlist in `local.conf`:
   - `settings.allowManagementFrom = ["0.0.0.0/0"]`
2. Kept API binding restricted to host loopback (`127.0.0.1:9993/tcp`) in compose to avoid widening external exposure.

Completed
1. Added repository-managed ZeroTier controller config file:
   - `docker/zerotier/local.conf`
2. Updated compose runtime wiring:
   - mounted `./docker/zerotier/local.conf` into `/var/lib/zerotier-one/local.conf` for `zerotier-controller`.
3. Updated Step 8.0 documentation/checklist state:
   - `README.md` now documents managed `allowManagementFrom` behavior and host-side verification commands.
   - `IMPLEMENTATION_PLAN.md` bumped to `Version: 1.5`, added Step 8.0 behavior for `local.conf`, and marked host-side `/status`, `/controller`, and `/controller/network` checks complete.
4. Resolved prior known bug:
   - host-side authenticated controller probes now return `200`.

Verification
1. `POSTGRES_HOST_PORT=55433 docker compose exec -T zerotier-controller sh -lc 'set -eu; t=$(cat /var/lib/zerotier-one/authtoken.secret | tr -d "\\r\\n"); code=$(curl -sS -o /tmp/in_status.json -w "%{http_code}" -H "X-ZT1-Auth: $t" http://127.0.0.1:9993/status || true); echo in_container_status_code=$code'` passed (`200`).
2. `POSTGRES_HOST_PORT=55433 docker compose restart zerotier-controller && POSTGRES_HOST_PORT=55433 docker compose ps zerotier-controller` passed (healthy after restart).
3. Host probes with token read from container secret:
   - `/status` returned `200`.
   - `/controller` returned `200`.
   - `/controller/network` returned `200`.

In progress
1. Phase 8 Step 8.1 to Step 8.5 remain open.

Next
1. Implement Phase 8 Step 8.1 readiness gate in API/worker provisioning path.
2. Implement Phase 8 Step 8.2 network reconciliation.
3. Implement Phase 8 Step 8.3 token lifecycle controls.

Known bugs / regressions
1. No known regressions in current Step 8.0 scope after host-side fix.

---

Update: 2026-02-10 (Lessons update for Step 8.0 host-side 401 root cause)

Completed
1. Updated `lessons.md` with a new operational lesson for ZeroTier controller management reachability:
   - symptom: host-side `/status` and `/controller` return `401` while in-container probe returns `200`,
   - root cause: missing/insufficient `allowManagementFrom` coverage for host-originated management path,
   - prevention rule: mount managed `local.conf` and keep host API binding on loopback.

In progress
1. No active implementation task in this update.

Next
1. Continue Phase 8 Step 8.1 readiness gate implementation.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 8 self-hosted controller lifecycle ownership implemented)

Completed
1. Implemented Phase 8 Step 8.1 readiness gate with auditable outcomes:
   - Added `app/provisioning/controller_lifecycle.py` with `/status` + `/controller` probes, deterministic lifecycle error codes/remediation metadata, and audit event emission.
   - Added startup preflight in `app/main.py` for `ZT_PROVIDER=self_hosted_controller`.
   - Added worker preflight gate in `app/provisioning/service.py` before provider authorization.
2. Implemented Phase 8 Step 8.2 required-network reconciliation:
   - Added reconciliation for `ZT_CONTROLLER_REQUIRED_NETWORK_IDS` with validate/create behavior and fail-closed preflight semantics.
3. Implemented Phase 8 Step 8.3 token lifecycle controls:
   - Added runtime token-file support (`ZT_CONTROLLER_AUTH_TOKEN_FILE`) via new helper `app/provisioning/controller_auth.py`.
   - Updated provider factory to resolve controller token from env or runtime secret file.
   - Added auditable token reload operation with immediate preflight validation.
4. Implemented Phase 8 Step 8.4 backup/restore workflows and validation drill:
   - Added controller state backup with retention policy enforcement.
   - Added restore workflow and restore-validation drill requiring readiness + reconciliation success.
5. Implemented Phase 8 Step 8.5 automated lifecycle coverage:
   - Added `tests/controller_lifecycle/` suite for readiness gate, network reconciliation, token reload, backup retention, restore validation, and startup gate strict/non-strict behavior.
   - Added provisioning lifecycle gate regression test in `tests/provisioning/test_service.py`.
6. Added lifecycle operations CLI:
   - `app/cli/controller_lifecycle.py` commands: `preflight`, `reload-token`, `backup`, `restore`, `restore-validate`.
7. Updated docs and plan state:
   - `.env.example`, `README.md`, `BACKEND_STRUCTURE.md`, and `IMPLEMENTATION_PLAN.md` updated for lifecycle env/config/verification.

Verification
1. `uv run pytest tests/controller_lifecycle -q` passed.
2. `uv run pytest tests/provisioning -q -k lifecycle` passed.
3. `uv run pytest tests/provisioning -q` passed.
4. `uv run ruff check .` passed.
5. `uv run mypy .` passed.
6. `uv run pytest -q` passed.
7. Manual compose sanity checks passed:
   - `POSTGRES_HOST_PORT=55433 docker compose up -d postgres redis zerotier-controller`
   - `POSTGRES_HOST_PORT=55433 docker compose ps postgres redis zerotier-controller`
   - `DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix uv run alembic upgrade head`
   - `... uv run python -m app.cli.controller_lifecycle preflight` (with controller token sourced from `authtoken.secret`) returned `controller lifecycle preflight succeeded`.

In progress
1. No active implementation tasks in Phase 8 scope.

Next
1. Execute remaining Phase 8 manual drills not covered in this environment:
   - forced readiness/auth failure gate behavior
   - backup -> restore drill on real controller state path.
2. Continue Phase 9 frontend hardening (`IMPLEMENTATION_PLAN.md` Step 9.1 to 9.4).
3. Keep Phase 11.5 and 11.6 release-stage self-hosted-only and DR drill sign-off as release blockers.

Known bugs / regressions
1. No functional regressions observed in automated test suite.

Notes
1. Startup lifecycle preflight uses FastAPI lifespan handlers (not deprecated `on_event` startup hooks).

---

Update: 2026-02-10 (Phase 8 follow-up: lifespan startup hook)

Completed
1. Replaced deprecated FastAPI `@app.on_event("startup")` lifecycle preflight hook with lifespan handler in `app/main.py`.
2. Preserved strict/non-strict startup gating behavior and existing lifecycle audit/preflight semantics.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/controller_lifecycle/test_startup.py -q` passed.
4. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Continue with remaining open phases (Phase 9 UI hardening; Phase 11 release/DR drills).

Known bugs / regressions
1. None identified in this follow-up.

---

Update: 2026-02-11 (Frontend planning stack alignment for Phase 9)

Decision
1. Frontend implementation stack is now explicitly:
   - ReactJS
   - TypeScript
   - shadcn-ui

Completed
1. Updated frontend stack definition in `TECH_STACK.md`:
   - removed legacy Jinja2/HTMX/Alpine frontend delivery assumptions,
   - documented ReactJS + TypeScript + shadcn-ui as the approved frontend delivery stack.
2. Updated implementation-plan frontend wording in `IMPLEMENTATION_PLAN.md`:
   - replaced legacy template-stack references with React/TypeScript/shadcn-ui terminology,
   - clarified Phase 9 step descriptions around React bootstrap and shadcn-ui component implementation.

Verification
1. Documentation-only diff reviewed for consistency across:
   - `TECH_STACK.md`
   - `IMPLEMENTATION_PLAN.md`
2. Confirmed no checklist completion state changes were required; frontend implementation steps remain open and blocked as documented.

In progress
1. No implementation code changes in this update.

Next
1. Start Phase 9 Step 9.1 frontend bootstrap using React + TypeScript.
2. Begin Phase 9 Step 9.2 component implementation with shadcn-ui while adhering to `FRONTEND_GUIDELINES.md`.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-11 (SPA/API architecture decisions + frontend phase split)

Decision
1. Frontend runtime model is now strict SPA:
   - client-side route ownership only,
   - frontend uses fetch/XHR APIs only (no backend workflow-page redirects),
   - production target is NGINX `web` container reverse-proxying to FastAPI `api`,
   - development target is Vite dev server with proxy to `localhost:8000`.
2. Approval mode policy is runtime-configurable via `runtime-config.yaml`:
   - `workflow.approval_mode=manual_admin|policy_auto` (`manual_admin` default),
   - detailed `policy_auto` guardrail expansion is out of scope for `v0.1.0`.
3. Frontend non-MVP items are explicitly deferred post-`v0.1.0`:
   - automated accessibility hardening,
   - i18n message catalogs,
   - advanced mobile/table scaling and audit timeline UX.

Completed
1. Updated design documents to align SPA/API architecture decisions:
   - `PRD.md` (scope, acceptance criteria, assumptions/open questions, DoD)
   - `APP_FLOW.md` (rewritten to SPA routes + API-only interactions + polling contract)
   - `TECH_STACK.md` (pinned frontend versions and NGINX/Vite runtime model)
   - `FRONTEND_GUIDELINES.md` (SPA runtime rules, polling rule, MVP accessibility scope)
   - `BACKEND_STRUCTURE.md` (SPA auth/workflow/admin API contract + runtime-config approval mode key)
   - `README.md` (runtime model and SPA-oriented auth flow guidance)
2. Rewrote implementation planning for frontend scope granularity:
   - `IMPLEMENTATION_PLAN.md` split former Phase 9 into:
     - Phase 9: API realignment + approval-mode config
     - Phase 10: SPA platform/delivery topology
     - Phase 11: core SPA workflow screens (MVP)
     - Phase 12: frontend MVP validation + deferred UX scope
   - Renumbered downstream phases:
     - Security/Observability -> Phase 13
     - Release readiness -> Phase 14
     - Route-server Option B TODO -> Phase 15
   - Added explicit blockers/dependencies for deferred/non-MVP frontend work.
3. Added runtime config example key:
   - `runtime-config.example.yaml` now includes `workflow.approval_mode`.

Verification
1. Documentation-only consistency review completed across updated docs.
2. Checklist/blocker references in `IMPLEMENTATION_PLAN.md` were aligned with new phase numbering and dependencies.
3. No implementation code or automated test execution changes were made in this update.

In progress
1. No active implementation tasks in this documentation update.

Next
1. Implement Phase 9 Step 9.1 to Step 9.6 (SPA API realignment + approval mode wiring).
2. Implement Phase 10 Step 10.1 to Step 10.5 (SPA workspace + NGINX/Vite topology).
3. Implement Phase 11 Step 11.1 to Step 11.6 (core SPA screens with polling).

Known bugs / regressions
1. None identified in this documentation-only update.

---

Update: 2026-02-11 (Phase ordering adjustment: backend-first before frontend)

Decision
1. Reordered the new frontend-era phases so backend/API prerequisites are delivered before frontend platform work:
   - Phase 9 is now `API Realignment for SPA and Approval-Mode Config`.
   - Phase 10 is now `SPA Platform and Delivery Topology`.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` phase ordering and step numbering to enforce backend-first sequencing.
2. Updated all dependent blocker links and references:
   - configurable approval-mode blockers now target Phase 9 Step 9.4 to Step 9.5,
   - SPA integration blockers now target Phase 10 Step 10.1 to Step 10.5 (plus Phase 11 screen work).

Verification
1. Reviewed `IMPLEMENTATION_PLAN.md` for reference consistency after renumbering.
2. Confirmed no checklist completion-state changes were required.

In progress
1. No implementation code changes in this update.

Next
1. Execute Phase 9 (API realignment + approval mode) before any frontend implementation.
2. Execute Phase 10 (SPA platform + delivery topology) after Phase 9 APIs are available.
3. Continue with Phase 11 SPA screens once Phase 9 and Phase 10 are in place.

Known bugs / regressions
1. None identified for this documentation-only adjustment.

---

Update: 2026-02-12 (Phase 9 API realignment + approval-mode runtime wiring completed)

Decision
1. Backend now treats SPA as the only workflow UI contract:
   - removed backend redirect/page dependencies for auth/workflow flows,
   - exposed JSON API endpoints for auth start/callback/local login/logout, onboarding context, and admin list/detail.
2. `workflow.approval_mode` is now runtime-driven from `runtime-config.yaml` (`manual_admin` default, `policy_auto` optional).
3. `policy_auto` remains intentionally bounded to existing ASN/network eligibility checks in `v0.1.0`.

Completed
1. Implemented Phase 9 Step 9.1 auth API surface in `app/routes/auth.py`:
   - `POST /api/v1/auth/peeringdb/start`
   - `POST /api/v1/auth/peeringdb/callback`
   - `POST /api/v1/auth/local/login`
   - `POST /api/v1/auth/logout`
   - Removed legacy redirect responses and `/auth/*` compatibility route behavior.
2. Implemented Phase 9 Step 9.2 workflow/admin API surface in `app/routes/workflow.py`:
   - `GET /api/v1/onboarding/context`
   - `GET /api/v1/admin/requests`
   - `GET /api/v1/admin/requests/{request_id}`
3. Implemented Phase 9 Step 9.3 and Step 9.7 backend cleanup:
   - removed backend `/error` contract and legacy page-route handlers (`/onboarding`, `/dashboard`, `/requests/{id}`, `/admin/requests*`).
4. Implemented Phase 9 Step 9.4 runtime-config approval-mode parsing in `app/config.py`:
   - reads `workflow.approval_mode` from `ZTIX_RUNTIME_CONFIG_PATH` (default `runtime-config.yaml`),
   - validates allowed values (`manual_admin`, `policy_auto`).
5. Implemented Phase 9 Step 9.5 approval behavior in `app/routes/workflow.py`:
   - `manual_admin`: preserves pending -> admin decision flow.
   - `policy_auto`: auto transitions eligible `pending` requests to `approved`, enqueues provisioning, and emits policy audit metadata.
6. Implemented Phase 9 Step 9.6 test realignment and expansion:
   - rewrote `tests/auth/test_auth_integration.py` for JSON callback/start semantics,
   - rewrote `tests/auth_local/test_auth_local_integration.py` for API login/logout and onboarding context behavior,
   - rewrote `tests/workflow/test_request_workflow_integration.py` for API-only onboarding/admin contracts + policy-auto assertions,
   - added runtime-config approval-mode tests in `tests/test_config.py`.
7. Updated supporting docs/config:
   - `.env.example` redirect URI default now SPA callback path and includes `ZTIX_RUNTIME_CONFIG_PATH`.
   - `README.md` wording updated to remove stale pre-Phase-9 note.
   - `IMPLEMENTATION_PLAN.md` updated to Version `1.9` with Phase 9 + Phase 4 Step 4.7 completion checkboxes.

Verification
1. `uv run ruff check app tests` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/auth -q tests/auth_local -q tests/workflow -q tests/test_config.py -q` passed.
4. `uv run pytest -q` passed.

In progress
1. No active implementation task in this update.

Next
1. Start Phase 10 Step 10.1 to Step 10.5 (frontend SPA workspace and NGINX/Vite runtime topology).
2. Begin Phase 11 core SPA screens after Phase 10 platform scaffolding is in place.

Known bugs / regressions
1. None detected in automated lint/type/test coverage for this scope.

---

Update: 2026-02-12 (Phase 10 SPA platform + delivery topology implemented)

Completed
1. Implemented Phase 10 Step 10.1 frontend workspace bootstrap:
   - Added `frontend/` with pinned React/React DOM (`19.2.4`), TypeScript (`5.9.3`), Vite (`7.3.1`), and npm package-manager metadata (`11.10.0`).
   - Added base Vite/TypeScript entry files (`index.html`, `vite.config.ts`, `tsconfig.json`, `src/main.tsx`).
2. Implemented Phase 10 Step 10.2 client-side router + shared app shell:
   - Added SPA route ownership for `/`, `/login`, `/auth/callback`, `/onboarding`, `/dashboard`, `/requests/:id`, `/admin/requests`, `/admin/requests/:id`.
   - Added shared responsive shell with sidebar/topbar and route placeholder surfaces for each required path.
   - Applied frontend guideline tokens (palette/spacing/type), motion baseline, and reduced-motion handling in `frontend/src/styles.css`.
3. Implemented Phase 10 Step 10.3 production web delivery assets:
   - Added `docker/nginx/default.conf` with SPA fallback (`try_files ... /index.html`) and `/api/` reverse proxy to `api:8000`.
   - Added `docker/web/Dockerfile` multi-stage build (frontend build + NGINX static serving).
4. Implemented Phase 10 Step 10.4 compose topology updates:
   - Added `api` and `web` services under `prod-like` profile in `docker-compose.yml`.
   - Preserved existing default infra-only local profile (`postgres`, `redis`, `zerotier-controller`).
5. Implemented Phase 10 Step 10.5 dev proxy + docs:
   - Added Vite dev proxy from `/api` to `http://localhost:8000`.
   - Updated `README.md` with local Vite run flow and production-like compose profile command.
   - Added compose host-port knobs in `.env.example` (`API_HOST_PORT`, `WEB_HOST_PORT`).
6. Updated planning/checklist state:
   - Marked Phase 10 steps and exit criteria complete in `IMPLEMENTATION_PLAN.md`.
   - Updated blockers referencing Phase 10 completion in snapshot/Phase 4 notes.

Verification
1. `npm install --package-lock-only` (inside `frontend/`) passed.
2. `docker compose --profile prod-like config` passed and shows `web` -> `api` topology wiring.
3. `npm ci` (inside `frontend/`) failed with `EAI_AGAIN` DNS resolution against `registry.npmjs.org` in this environment.
4. `npm run build` (inside `frontend/`) is blocked until `npm ci` succeeds (`vite` binary unavailable without install).

In progress
1. No active implementation tasks in Phase 10 scope.

Next
1. Resolve environment DNS/network access so `frontend` install/build verification can be rerun (`npm ci`, `npm run build`).
2. Start Phase 11 Step 11.1 to Step 11.6 (functional login/onboarding/operator/admin SPA screens).

Known bugs / regressions
1. No functional regressions identified in backend paths touched by this update.
2. Frontend build verification remains blocked by environment network resolution (`EAI_AGAIN`).

---

Update: 2026-02-12 (Phase 10 frontend verification blocker cleared)

Completed
1. Re-ran frontend setup after network-access update:
   - `npm ci` in `frontend/` now succeeds.
   - `npm run build` in `frontend/` now succeeds and emits production assets under `frontend/dist`.
2. Re-validated compose topology contract:
   - `docker compose --profile prod-like config` still resolves `web -> api` wiring.
3. Performed Vite runtime sanity check:
   - `npm run dev -- --host 127.0.0.1 --port 4173 --strictPort` started successfully (`VITE v7.3.1 ready`).
4. Updated implementation checklist state:
   - Marked Phase 10 verification items `npm ci` and `npm run build` as complete in `IMPLEMENTATION_PLAN.md`.

Verification
1. `npm ci` (inside `frontend/`) passed.
2. `npm run build` (inside `frontend/`) passed.
3. `timeout 15s npm run dev -- --host 127.0.0.1 --port 4173 --strictPort` confirmed Vite startup.
4. `docker compose --profile prod-like config` passed.

In progress
1. No active implementation work in Phase 10 scope.

Next
1. Start Phase 11 Step 11.1 to Step 11.6 (functional SPA screens and API integration).
2. Optionally align local npm runtime to `11.10.0` to remove non-blocking engine warning in this environment.

Known bugs / regressions
1. No new regressions detected in this follow-up verification pass.
2. `npm ci` reports non-blocking `EBADENGINE` warning because host npm is `10.8.2` while workspace metadata targets npm `11.10.0`.

---

Update: 2026-02-12 (Frontend design guideline direction realignment)

Completed
1. Updated `FRONTEND_GUIDELINES.md` visual system to a marble-inspired operational console direction with plain white/light primary surfaces.
2. Replaced typography guidance with:
   - `Noto Sans` as primary UI typeface.
   - `Noto Sans Mono` for `asn`, `zt_network_id`, `node_id`, and request IDs.
3. Replaced legacy color palette with canonical marble tokens:
   - surface (`paperWhite`, `softWhite`)
   - stone (`mistBlue`, `powderBlue`, `blueFog`)
   - depth (`graphiteSlate`, `deepTealSlate`, `harborBlue`)
   - vein accents (`clay`, `taupe`)
4. Added explicit color/surface behavior constraints:
   - warm-beige accents as primary accent system (target under ~10% of visible UI),
   - blue reserved for small intentional emphasis only,
   - light-first surfaces, subtle mineral gradients on secondary areas, thin borders, and soft shadows.
5. Aligned component-level guidance (buttons, status badges, tables, forms) with the new token and accent usage model to remove conflicting legacy color directives.

Verification
1. Manual document review confirms prior `Sora`/`Source Sans 3`/`IBM Plex Mono` and legacy palette tokens were removed from `FRONTEND_GUIDELINES.md`.
2. Manual document review confirms required visual-direction, typography, marble-token, and color/surface behavior sections are present.

In progress
1. No active implementation task in this update.

Next
1. Apply the updated tokens and typography in Phase 11 SPA screen implementation work.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-12 (Phase 11 core SPA workflow screens implemented)

Completed
1. Implemented Phase 11 Step 11.1 login and callback flows in SPA:
   - Added functional `/login` with PeeringDB start action and local credential form (`POST /api/v1/auth/peeringdb/start`, `POST /api/v1/auth/local/login`).
   - Added functional `/auth/callback` processor with inline callback error handling (`POST /api/v1/auth/peeringdb/callback`) and onboarding redirect on success.
2. Implemented Phase 11 Step 11.2 onboarding workflow:
   - Added `/onboarding` context loading (`GET /api/v1/onboarding/context`) and request submission (`POST /api/v1/requests`).
   - Added inline handling for duplicate active-request conflicts, eligibility-blocked state, and submission errors.
3. Implemented Phase 11 Step 11.3 operator routes with polling:
   - Added `/dashboard` request table (`GET /api/v1/requests`) with bounded HTTP polling refresh.
   - Added `/requests/:id` detail page (`GET /api/v1/requests/{request_id}`) with bounded polling and membership/error surfaces.
4. Implemented Phase 11 Step 11.4 admin routes and actions:
   - Added `/admin/requests` queue with filters (`status`, `asn`, `zt_network_id`, `min_age_minutes`) and polling (`GET /api/v1/admin/requests`).
   - Added `/admin/requests/:id` review page with approve/reject/retry actions and audit timeline (`GET/POST /api/v1/admin/requests/{id}*`).
5. Implemented Phase 11 Step 11.5/11.6 table primitives and MVP states:
   - Added local shadcn-style table primitive components in `frontend/src/components/ui/table.tsx` and wired them into operator/admin tables.
   - Added minimum-viable empty/error states across login, onboarding, dashboard, request detail, admin queue, and admin detail views.
6. Realigned frontend visual system to current guideline tokens:
   - Replaced legacy typography/colors with `Noto Sans` + `Noto Sans Mono` and marble token palette in `frontend/src/styles.css`.
   - Preserved mobile-first shell behavior, breakpoint rules, page-enter motion, row stagger motion, and reduced-motion fallback.
7. Updated checklist state in `IMPLEMENTATION_PLAN.md`:
   - Marked Phase 11 Step 11.1 through Step 11.6 complete.
   - Marked Phase 11 exit criteria complete.
   - Added explicit blocked annotations for manual browser verification tasks in this environment.

Verification
1. `npm run build` (inside `frontend/`) passed.
2. `timeout 15s npm run dev -- --host 127.0.0.1 --port 4173 --strictPort` failed in this environment with `EPERM` bind restriction (sandbox limitation, not frontend compile failure).

In progress
1. No active implementation task in this update.

Next
1. Run manual SPA walkthrough in an interactive browser: `/login` -> `/auth/callback` -> `/onboarding` -> `/requests/:id` -> `/admin/requests/:id`.
2. Validate live polling transitions end-to-end against running API/worker stack.
3. Start Phase 12 Step 12.1 integration smoke coverage for frontend/API flows.

Known bugs / regressions
1. No build-time regressions detected in frontend bundle output for this scope.
2. Local dev server bind check is blocked by sandbox port permissions in this execution environment.

---

Update: 2026-02-12 (Phase 11 follow-up: session refresh loop guard)

Completed
1. Fixed SPA session-refresh callback stability in `frontend/src/App.tsx`:
   - removed `session.status` from `refreshSession` callback dependencies,
   - removed eager `setSession({status:"loading"})` inside refresh path,
   - prevents repeat session-check loops caused by callback identity churn.

Verification
1. `npm run build` (inside `frontend/`) passed after refresh-loop fix.

In progress
1. No active implementation tasks in this follow-up.

Next
1. Manual browser walkthrough and polling validation remain pending as documented in Phase 11 verification blockers.

Known bugs / regressions
1. No new regressions detected in frontend build output for this follow-up fix.

---

Update: 2026-02-12 (Compose env sync: ZeroTier controller secret alignment)

Completed
1. Reviewed `docker-compose.yml` environment-variable handling for `zerotier-controller` and `api` against `.env` usage.
2. Synchronized controller secret inputs so `.env` is authoritative for both services:
   - added `ZT_CONTROLLER_AUTH_TOKEN` to `zerotier-controller` service env.
   - added `ZT_CONTROLLER_AUTH_TOKEN` to `api` service env.
   - changed `ZT_CONTROLLER_AUTH_TOKEN_FILE` in `api` from hardcoded file path to `.env`-driven value (`${ZT_CONTROLLER_AUTH_TOKEN_FILE:-}`).
3. Preserved existing controller network wiring and volume mounts while removing the hardcoded token-file override in compose.

Verification
1. `docker compose --profile prod-like config` passed and shows synchronized `ZT_CONTROLLER_AUTH_TOKEN` wiring for both `zerotier-controller` and `api`.
2. Verified `api` now receives `ZT_CONTROLLER_AUTH_TOKEN_FILE` from `.env` interpolation instead of a fixed compose literal.

In progress
1. No active implementation tasks in this update.

Next
1. If desired, run a runtime smoke check with the prod-like profile to confirm API preflight succeeds under your intended `.env` token mode (direct token vs token file).

Known bugs / regressions
1. No regressions identified in compose configuration rendering for this change.

---

Update: 2026-02-12 (YAML-backed app config loading)

Completed
1. Reworked `app/config.py` so `AppSettings` loads runtime values from `runtime-config.yaml` (YAML) instead of `os.getenv` lookups.
2. Kept existing defaults/clamps for session, auth, provisioning, and route-server settings when YAML keys are missing.
3. Added a lightweight YAML parser path in config loading and preserved `AppSettings.from_env(...)` as a compatibility alias to YAML loading.
4. Updated `tests/test_config.py` to validate YAML-driven behavior (scopes normalization, local auth bounds, provider settings, approval mode parsing).
5. Updated `IMPLEMENTATION_PLAN.md` Phase 9 checklist with completed Step 9.8 and verification entry.

Verification
1. `uv run ruff check app/config.py tests/test_config.py` passed.
2. `uv run pytest tests/test_config.py -q` passed.

In progress
1. No active tasks in this update.

Next
1. If needed, update deployment docs to show where secret values should live now that app settings are YAML-sourced.
2. Run full test suite to ensure no downstream runtime assumptions still depend on env-driven config.

Known bugs / regressions
1. None identified from targeted config tests.

Update addendum: 2026-02-12 (YAML config follow-up test alignment)

Completed
1. Updated `tests/cli/test_users_cli.py` to source password-policy test inputs from a temporary `runtime-config.yaml` instead of environment variables.

Verification
1. `uv run ruff check app/config.py tests/test_config.py tests/cli/test_users_cli.py` passed.
2. `uv run pytest tests/test_config.py tests/cli/test_users_cli.py -q` passed.

Update addendum: 2026-02-12 (Config loader switched to PyYAML)

Completed
1. Replaced custom YAML parsing in `app/config.py` with `PyYAML` (`yaml.safe_load`) for runtime config loading.
2. Kept existing defaults, clamping, and compatibility alias (`AppSettings.from_env` -> `from_yaml`).
3. Added pinned dependency `pyyaml==6.0.2` in `pyproject.toml` and documented `PyYAML 6.0.2` in `TECH_STACK.md`.
4. Synced environment to update lock/install set.

Verification
1. `uv sync --dev` passed.
2. `uv run ruff check app/config.py tests/test_config.py tests/cli/test_users_cli.py` passed.
3. `uv run pytest tests/test_config.py tests/cli/test_users_cli.py -q` passed.

Update addendum: 2026-02-12 (Config reader helper simplification)

Completed
1. Removed `_get_*` config accessor methods from `app/config.py`.
2. Refactored `AppSettings.from_yaml` to read typed values directly from parsed YAML dictionaries, assuming correct YAML data types.
3. Kept existing default fallbacks, approval-mode validation, and normalization of PeeringDB scopes.

Verification
1. `uv run ruff check app/config.py tests/test_config.py tests/cli/test_users_cli.py` passed.
2. `uv run pytest tests/test_config.py tests/cli/test_users_cli.py -q` passed.

---

Update: 2026-02-12 (Planning docs: canonical ZeroTier network ID prefix+suffix model)

Completed
1. Updated planning docs to enforce a canonical ZeroTier ID model and reduce repeated full network-ID literals:
   - `PRD.md` v0.7
   - `APP_FLOW.md` v0.7
   - `BACKEND_STRUCTURE.md` v0.7
   - `IMPLEMENTATION_PLAN.md` v2.2
2. Documented the rule that, in self-hosted mode, Python must read controller prefix from `GET /controller` and compose full network IDs from configured 6-char suffixes.
3. Added explicit anti-repetition guidance: runtime config should carry suffixes, not repeated full network IDs.
4. Added/expanded the open backend integration task under Phase 8:
   - `IMPLEMENTATION_PLAN.md` Step 8.6 is now open for controller-prefix discovery + suffix expansion + validation/audit behavior.
5. Updated `runtime-config.example.yaml` to show suffix-only configuration key (`required_network_suffixes`).

In progress
1. No implementation code changes in this update (planning/docs only).

Next
1. Implement Phase 8 Step 8.6 in Python lifecycle logic.
2. Add lifecycle tests for suffix parsing/composition and invalid/duplicate suffix failures.
3. Migrate runtime config usage from full-ID lists to suffix-only keys and remove legacy repetition paths.

Known bugs / regressions
1. Current Python implementation still expects full required network IDs; suffix-only behavior is documented and tracked but not implemented yet.

---

Update: 2026-02-12 (Phase 8 Step 8.6 canonical suffix-derived required networks implemented)

Completed
1. Implemented canonical required-network derivation in lifecycle preflight:
   - reads controller metadata ID from `/controller`,
   - derives controller prefix from first 10 lowercase hex characters,
   - expands `required_network_suffixes` to full 16-char IDs before reconciliation.
2. Added strict suffix validation and guardrails:
   - rejects malformed suffixes,
   - rejects duplicate suffixes,
   - rejects mixed repetition when a suffix-expanded ID is also present in legacy full-ID config.
3. Added lifecycle audit coverage for derivation outcomes:
   - `controller_lifecycle.required_network_derivation.succeeded`
   - `controller_lifecycle.required_network_derivation.failed`
   with prefix/expanded-ID metadata.
4. Extended runtime config loading for `required_network_suffixes` while retaining legacy `required_network_ids` compatibility input.
5. Updated active runtime config example (`runtime-config.yaml`) to suffix-only required-network key.
6. Added/updated automated tests for suffix derivation, validation failures, mixed repetition rejection, and config parsing.
7. Updated `IMPLEMENTATION_PLAN.md` checklist status:
   - Step 8.6 marked complete.
   - Phase 8 suffix-expansion verification check marked complete.
   - Phase 8 release-profile blocker narrowed to Phase 14 release validation only.

Verification
1. `uv run pytest tests/test_config.py -q` passed.
2. `uv run pytest tests/controller_lifecycle/test_lifecycle.py -q` passed.
3. `uv run pytest tests/controller_lifecycle -q` passed.
4. `uv run pytest tests/controller_lifecycle -q -k suffix` passed.
5. `uv run pytest tests/provisioning -q -k lifecycle` passed.
6. `uv run ruff check app/config.py app/provisioning/controller_lifecycle.py tests/test_config.py tests/controller_lifecycle/test_lifecycle.py` passed.
7. `uv run mypy .` passed.

In progress
1. No active implementation tasks in Step 8.6 scope.

Next
1. Execute Phase 14 staging/release validation for self-hosted profile behavior without `ZT_CENTRAL_API_TOKEN`.
2. Continue remaining SPA/frontend phases and manual checks tracked in `IMPLEMENTATION_PLAN.md`.

Known bugs / regressions
1. No new regressions detected from Step 8.6 implementation scope.

---

Update: 2026-02-12 (Phase 8 preflight SQL network-id reconciliation)

Completed
1. Updated `run_controller_lifecycle_preflight` in `app/provisioning/controller_lifecycle.py` to sync `zt_network` rows after successful controller network reconciliation.
2. Added lifecycle DB sync behavior:
   - upsert required reconciled network IDs (`existing_network_ids + created_network_ids`) into `zt_network`,
   - force required rows to active,
   - delete stale non-required `zt_network` rows.
3. Added fail-closed reconciliation error handling for SQL sync failures (integrity/DB errors) so preflight does not silently keep drift.
4. Added regression coverage in `tests/controller_lifecycle/test_lifecycle.py`:
   - `test_preflight_syncs_zt_network_rows_to_reconciled_ids` verifies stale-row pruning and required-row upsert behavior.

Verification
1. `uv run pytest tests/controller_lifecycle/test_lifecycle.py -q` passed.
2. `uv run ruff check app/provisioning/controller_lifecycle.py tests/controller_lifecycle/test_lifecycle.py` passed.
3. `uv run mypy app/provisioning/controller_lifecycle.py tests/controller_lifecycle/test_lifecycle.py` passed.

In progress
1. No active implementation task in this update.

Next
1. Optionally run full lifecycle + provisioning suites to validate no downstream behavior regressions.

Known bugs / regressions
1. None identified in this scoped change.

---

Update: 2026-02-12 (Compose: added Celery worker container)

Completed
1. Added `worker` service to `docker-compose.yml` for Celery execution in the `prod-like` profile.
2. Reused the existing app image/runtime wiring and set Celery command to `uv run celery --app app.provisioning.tasks:celery_app worker --loglevel=INFO`.
3. Aligned docs for production-like topology updates:
   - `README.md` now runs `web` + `worker` and documents worker startup.
   - `IMPLEMENTATION_PLAN.md` Phase 10 Step 10.4 text now reflects `web` + `api` + `worker` compose topology.

Verification
1. `docker compose --profile prod-like config` passed.

In progress
1. No active implementation task in this update.

Next
1. Optionally run `docker compose --profile prod-like up --build web worker` and verify worker consumes queued provisioning tasks in a live flow.

Known bugs / regressions
1. None identified from file-level changes.

---

Update: 2026-02-12 (Compose: Celery worker broker startup fix)

Completed
1. Updated `docker-compose.yml` `worker` command to set an explicit Celery broker:
   - added `--broker=redis://redis:6379/0`
2. Resolved worker startup fallback to default AMQP (`amqp://guest@127.0.0.1:5672//`) that caused connection-refused failures.

Verification
1. `docker compose config` passed and rendered the worker command with `--broker=redis://redis:6379/0`.
2. Smoke-validated container startup with host-port override in this environment:
   - `POSTGRES_HOST_PORT=55433 docker compose up -d redis worker`
   - `POSTGRES_HOST_PORT=55433 docker compose logs --tail=200 worker`
3. Worker logs confirm Redis transport and successful broker connection:
   - `transport: redis://redis:6379/0`
   - `Connected to redis://redis:6379/0`

In progress
1. No active implementation task in this update.

Next
1. Optionally run a full async flow (`api` enqueue -> `worker` consume) in prod-like compose profile to verify end-to-end task dispatch behavior.

Known bugs / regressions
1. None identified for the worker broker startup fix.

---

Update: 2026-02-12 (Join request uniqueness scoped by node_id)

Completed
1. Updated join-request active uniqueness to treat `node_id` as part of the request identity.
2. Added node-aware partial unique indexes in ORM metadata:
   - `uq_join_request_active_per_asn_network_with_node` on (`asn`, `zt_network_id`, `node_id`) for active statuses when `node_id IS NOT NULL`.
   - `uq_join_request_active_per_asn_network_without_node` on (`asn`, `zt_network_id`) for active statuses when `node_id IS NULL`.
3. Added Alembic migration `20260212_0003` to replace the old active-request index with the two node-scoped indexes (and reversible downgrade behavior).
4. Updated duplicate-conflict repository and workflow handling to look up existing active requests by (`asn`, `zt_network_id`, `node_id`) and return node-aware conflict details.
5. Updated DB and workflow tests to cover:
   - allowing same ASN/network with different `node_id`,
   - rejecting exact duplicate ASN/network/`node_id`,
   - preserving duplicate protection for null-`node_id` active requests.
6. Updated planning/contract docs (`PRD.md`, `APP_FLOW.md`, `BACKEND_STRUCTURE.md`) to reflect the new invariant.

Verification
1. `uv run pytest tests/db/test_join_request_invariants.py -q` passed.
2. `uv run pytest tests/workflow/test_request_workflow_integration.py -q` passed.
3. `uv run ruff check app/db/models.py app/repositories/join_requests.py app/repositories/errors.py app/routes/workflow.py tests/db/test_join_request_invariants.py tests/workflow/test_request_workflow_integration.py alembic/versions/20260212_0003_join_request_node_scoped_active_uniqueness.py` passed.
4. `uv run mypy app/repositories/join_requests.py app/routes/workflow.py tests/db/test_join_request_invariants.py tests/workflow/test_request_workflow_integration.py` passed.
5. `uv run env DATABASE_URL=sqlite:////tmp/zt_ix_node_scope.db alembic upgrade head && uv run env DATABASE_URL=sqlite:////tmp/zt_ix_node_scope.db alembic downgrade base && uv run env DATABASE_URL=sqlite:////tmp/zt_ix_node_scope.db alembic upgrade head` passed.

In progress
1. No active implementation task in this update.

Next
1. Optional: run full backend test suite for broader regression coverage.
2. Optional: run migration lifecycle against local PostgreSQL profile to mirror production dialect behavior.

Known bugs / regressions
1. None identified in this scoped change.

Update addendum: 2026-02-12 (PostgreSQL migration-path verification for node-scoped request uniqueness)

Completed
1. Started local PostgreSQL container on host port `55433` and validated the new migration on PostgreSQL dialect.
2. Verified targeted migration reversibility for the new revision only:
   - `20260212_0003 -> 20260210_0002 -> 20260212_0003` succeeded.
3. Stopped the temporary PostgreSQL container after verification.

Verification
1. `POSTGRES_HOST_PORT=55433 docker compose up -d postgres` passed.
2. `uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic downgrade 20260210_0002 && uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic upgrade head` passed.
3. `docker compose stop postgres` passed.

Known bugs / regressions
1. Existing historical migration downgrade path issue (pre-existing to this change): full `head -> base` downgrade on PostgreSQL fails in `20260210_0002` downgrade when attempting to restore `app_user.peeringdb_user_id` to `NOT NULL` while null-valued rows exist.

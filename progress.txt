ZT-IX Brainstorming Progress
Date: 2026-02-10

What I looked up
1. PeeringDB OAuth/OIDC docs:
   - https://docs.peeringdb.com/howto/oauth/
   - https://auth.peeringdb.com/register/
2. ZeroTier API/auth docs:
   - https://docs.zerotier.com/api/
   - https://docs.zerotier.com/api-central/
   - https://docs.zerotier.com/api-tokens/
   - https://docs.zerotier.com/sso/

Key findings that affect architecture
1. PeeringDB supports OpenID Connect login and documented OAuth2/OIDC app registration.
2. PeeringDB access tokens can call PeeringDB APIs with `Authorization: Bearer <access_token>`.
3. PeeringDB user permission introspection is available from:
   - `https://auth.peeringdb.com/oauth2/userinfo?scope=peeringdb.manage%20peeringdb.read`
4. ZeroTier Central API is token-based (bearer API token), not a third-party OAuth login provider for this use case.
5. ZeroTier docs also include OIDC SSO, but that is for ZeroTier service/device SSO, not app-to-Central API auth.

How I would build this app
1. Backend first (FastAPI + PostgreSQL)
   - Implement PeeringDB OIDC Authorization Code flow with PKCE.
   - Persist sessions and map PeeringDB user ID to local user record.
   - Exchange code for tokens server-side and store access token encrypted at rest.
2. Identity and policy layer
   - Query PeeringDB API for ASN/network objects allowed for the user.
   - Use PeeringDB userinfo/permissions scope checks for authorization gates.
   - Enforce allowlist rules (for example, accepted ASN ranges or explicit ASN whitelist).
3. ZeroTier orchestration layer
   - Use a service-owned ZeroTier Central API token (stored as secret).
   - For each approved ASN/user request:
     - create or update member authorization in target ZeroTier network,
     - assign managed IP if needed,
     - write provisioning status/audit events.
4. UX flow
   - Login with PeeringDB -> choose ASN -> submit join request -> admin approval (or policy auto-approval) -> show active member state.
   - Add clear status states: `pending`, `approved`, `provisioning`, `active`, `failed`.
5. Operational safety
   - Idempotent provisioning jobs.
   - Retry with exponential backoff for ZeroTier API errors.
   - Full audit log for all auth, approvals, and provisioning changes.

Initial milestone breakdown
1. Milestone 1: Auth and onboarding
   - OIDC login, callback, session handling, ASN fetch.
2. Milestone 2: Membership workflow
   - Join request model, admin approve/reject, status tracking.
3. Milestone 3: ZeroTier provisioning
   - Central API client, member authorization, reconciliation task.
4. Milestone 4: Hardening
   - Tests, observability, rate limits, CSRF/session security.

Open decisions before implementation
1. Should approvals be fully automatic for verified ASN ownership or always admin-gated?
2. Is one shared ZeroTier network sufficient, or does each ASN get isolated network segments?
3. What exact PeeringDB scopes should be considered minimum for production?

---

Update: 2026-02-10

Decision
1. ZeroTier provisioning will use a provider abstraction with two supported modes:
   - `central` (ZeroTier Central API)
   - `self_hosted_controller` (self-hosted ZeroTier controller API via local service auth)
2. Scope remains control-plane independence only; automated roots/planet management is out of scope for phase 1.

Completed
1. Updated planning docs to remove Central-only assumptions and define provider-based provisioning:
   - `PRD.md`
   - `APP_FLOW.md`
   - `TECH_STACK.md`
   - `BACKEND_STRUCTURE.md`
   - `IMPLEMENTATION_PLAN.md`

In progress
1. No implementation code yet. Documentation alignment is complete.

Next
1. Implement Phase 5 Step 5.1 through Step 5.4:
   - provider interface
   - `central` adapter
   - `self_hosted_controller` adapter
   - Celery task provider selection from `ZT_PROVIDER`
2. Add provider contract tests and adapter selection tests.
3. Update `.env.example` for provider mode and credentials.

Known risks / notes
1. Self-hosted controller endpoint/auth behavior must be validated in integration tests before production rollout.
2. Provider API differences can cause behavior drift without strict contract tests.

---

Update: 2026-02-10 (Phase 1 bootstrap complete)

Completed
1. Implemented Phase 1 Step 1.1 project structure bootstrap:
   - Added `app/`, `tests/`, and `alembic/` scaffolding.
   - Added runnable FastAPI app entrypoint with health endpoint.
2. Implemented Phase 1 Step 1.2 pinned dependency setup:
   - Updated `pyproject.toml` with exact runtime and dev versions from `TECH_STACK.md`.
   - Synced environment with `uv sync --dev` and generated `uv.lock`.
3. Implemented Phase 1 Step 1.3 quality tooling configuration:
   - Added `ruff`, `mypy`, and `pytest` configuration in `pyproject.toml`.
   - Added baseline API health test.
4. Implemented Phase 1 Step 1.4 environment bootstrap:
   - Added `.env.example` with required secrets, endpoint settings, and provider mode variables.
5. Added missing `lessons.md` stub as required by `AGENTS.md`.
6. Updated bootstrap run instructions in `README.md`.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 2 Step 2.1: SQLAlchemy model layer per `BACKEND_STRUCTURE.md`.
2. Start Phase 2 Step 2.2: Alembic initial migration from modeled schema.
3. Add Phase 2 database invariant tests (unique active request constraint and status transition rules).

Known bugs / regressions
1. None identified in Phase 1 bootstrap scope.

Notes
1. `uv run` and `uv sync` required elevated permissions in this environment due cache directory restrictions.

---

Update: 2026-02-10 (Local dependency profile decision documented)

Decision
1. Selected for local development dependencies:
   - Docker Compose for PostgreSQL and Redis only.
   - API/worker/test processes run directly with `uv run`.

Completed
1. Documented the selected local dependency profile in:
   - `TECH_STACK.md`
   - `IMPLEMENTATION_PLAN.md`
   - `README.md`

In progress
1. No implementation code changes in this update.

Next
1. Start Phase 2 Step 2.1: SQLAlchemy model layer per `BACKEND_STRUCTURE.md`.
2. Start Phase 2 Step 2.2: Alembic initial migration from modeled schema.
3. Add Phase 2 database invariant tests (unique active request constraint and status transition rules).

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 2 data and migration foundation complete)

Completed
1. Implemented Phase 2 Step 2.1 SQLAlchemy data model layer per `BACKEND_STRUCTURE.md`:
   - Added DB base/metadata conventions and status enums.
   - Added ORM models for `app_user`, `user_asn`, `zt_network`, `join_request`, `zt_membership`, `oauth_state_nonce`, and `audit_event`.
   - Added DB session helpers.
2. Implemented Phase 2 Step 2.2 initial Alembic migration:
   - Wired Alembic to SQLAlchemy metadata.
   - Added initial revision creating enum, tables, indexes, and active-request partial unique index.
3. Implemented Phase 2 Step 2.3 repository layer:
   - Added repositories for users, user ASN mappings, join requests, memberships, and audit events.
   - Added repository error types for duplicate active request and invalid state transition.
4. Implemented Phase 2 Step 2.4 DB invariant tests:
   - Added DB fixtures and tests covering:
     - one active request per (`asn`, `zt_network_id`)
     - allowed/forbidden request status transitions

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/db -q` passed.
4. `uv run pytest -q` passed.
5. Migration lifecycle verified with `uv run env DATABASE_URL=sqlite:////tmp/zt_ix_phase2.db alembic upgrade/downgrade/upgrade`.

In progress
1. No active implementation tasks.

Next
1. Start Phase 3 Step 3.1: `/auth/login` state/nonce/PKCE generation.
2. Start Phase 3 Step 3.2: `/auth/callback` state validation and token exchange path.
3. Add auth integration tests for success/failure callback branches.

Known bugs / regressions
1. `alembic upgrade head` against local PostgreSQL could not be validated in this session because PostgreSQL on `localhost:5432` was unavailable.

---

Update: 2026-02-10 (Phase 2 PostgreSQL verification gap closed)

Completed
1. Added concrete local dependency orchestration for documented infrastructure profile:
   - Added `docker-compose.yml` with pinned images for PostgreSQL `16.6` and Redis `7.4.1`.
   - Set PostgreSQL host mapping default to `5433` (`${POSTGRES_HOST_PORT:-5433}:5432`) to avoid local `5432` conflicts.
2. Aligned local default DB connection settings to host port `5433`:
   - `.env.example`
   - `app/db/session.py`
   - `alembic.ini`
3. Added README run instructions for starting local dependencies and documented the `5433` host-port expectation.
4. Fixed a PostgreSQL migration defect in `alembic/versions/20260210_0001_phase2_data_foundation.py`:
   - Removed redundant explicit enum creation causing duplicate `request_status` type creation during `upgrade`.
5. Completed previously blocked PostgreSQL migration verification using alternate host port override in this environment:
   - Ran upgrade/downgrade/upgrade successfully against running local PostgreSQL.

Verification
1. `docker compose ps` (PostgreSQL and Redis healthy).
2. `uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic upgrade head && uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic downgrade base && uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic upgrade head` passed.
3. `uv run ruff check .` passed.
4. `uv run mypy .` passed.
5. `uv run pytest tests/db -q` passed.
6. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 3 Step 3.1: `/auth/login` state/nonce/PKCE generation.
2. Start Phase 3 Step 3.2: `/auth/callback` state validation and token exchange path.
3. Add auth integration tests for success/failure callback branches.

Known bugs / regressions
1. None identified in Phase 2 scope after PostgreSQL verification.

Notes
1. In this environment, host port `5433` was already allocated; verification used `POSTGRES_HOST_PORT=55433` while keeping repo defaults at `5433`.
2. Added an AGENTS.md non-negotiable to keep escalation-sensitive commands on consistent prefixes (for example, `docker compose`, `uv run`) to reduce repeated approvals.

---

Update: 2026-02-10 (Implementation plan updated for route-server Option A + Option B TODO)

Decision
1. Route-server automation is now planned as Option A in active scope:
   - Worker-driven SSH orchestration to remote Ubuntu/Linux route servers.
   - Explicit generated BIRD peer configuration per ASN on every configured route server.
   - ROA/RPKI validation required in the BIRD policy/configuration path for generated peers.
2. Option B (persisted per-route-server state model) is deferred and tracked as a TODO phase after Phase 8.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version 0.3:
   - Added Option A assumptions/requirements.
   - Expanded Phase 5 steps and exit criteria for route-server sync via SSH.
   - Added explicit per-ASN peer generation requirement.
   - Added ROA/RPKI validation requirement.
   - Added post-Phase-8 TODO section for Option B.

In progress
1. No implementation code changes in this update.

Next
1. Align `PRD.md`, `APP_FLOW.md`, and `BACKEND_STRUCTURE.md` with the new route-server Option A scope before implementation begins.
2. Define environment/config contract for route-server SSH fanout (inventory, auth, command policy).
3. Start Phase 3 auth implementation once scope docs are aligned.

Known bugs / regressions
1. None identified for this documentation-only update.

ZT-IX Brainstorming Progress
Date: 2026-02-10

What I looked up
1. PeeringDB OAuth/OIDC docs:
   - https://docs.peeringdb.com/howto/oauth/
   - https://auth.peeringdb.com/register/
2. ZeroTier API/auth docs:
   - https://docs.zerotier.com/api/
   - https://docs.zerotier.com/api-central/
   - https://docs.zerotier.com/api-tokens/
   - https://docs.zerotier.com/sso/

Key findings that affect architecture
1. PeeringDB supports OpenID Connect login and documented OAuth2/OIDC app registration.
2. PeeringDB access tokens can call PeeringDB APIs with `Authorization: Bearer <access_token>`.
3. PeeringDB user permission introspection is available from:
   - `https://auth.peeringdb.com/oauth2/userinfo?scope=peeringdb.manage%20peeringdb.read`
4. ZeroTier Central API is token-based (bearer API token), not a third-party OAuth login provider for this use case.
5. ZeroTier docs also include OIDC SSO, but that is for ZeroTier service/device SSO, not app-to-Central API auth.

How I would build this app
1. Backend first (FastAPI + PostgreSQL)
   - Implement PeeringDB OIDC Authorization Code flow with PKCE.
   - Persist sessions and map PeeringDB user ID to local user record.
   - Exchange code for tokens server-side and store access token encrypted at rest.
2. Identity and policy layer
   - Query PeeringDB API for ASN/network objects allowed for the user.
   - Use PeeringDB userinfo/permissions scope checks for authorization gates.
   - Enforce allowlist rules (for example, accepted ASN ranges or explicit ASN whitelist).
3. ZeroTier orchestration layer
   - Use a service-owned ZeroTier Central API token (stored as secret).
   - For each approved ASN/user request:
     - create or update member authorization in target ZeroTier network,
     - assign managed IP if needed,
     - write provisioning status/audit events.
4. UX flow
   - Login with PeeringDB -> choose ASN -> submit join request -> admin approval (or policy auto-approval) -> show active member state.
   - Add clear status states: `pending`, `approved`, `provisioning`, `active`, `failed`.
5. Operational safety
   - Idempotent provisioning jobs.
   - Retry with exponential backoff for ZeroTier API errors.
   - Full audit log for all auth, approvals, and provisioning changes.

Initial milestone breakdown
1. Milestone 1: Auth and onboarding
   - OIDC login, callback, session handling, ASN fetch.
2. Milestone 2: Membership workflow
   - Join request model, admin approve/reject, status tracking.
3. Milestone 3: ZeroTier provisioning
   - Central API client, member authorization, reconciliation task.
4. Milestone 4: Hardening
   - Tests, observability, rate limits, CSRF/session security.

Open decisions before implementation
1. Should approvals be fully automatic for verified ASN ownership or always admin-gated?
2. Is one shared ZeroTier network sufficient, or does each ASN get isolated network segments?
3. What exact PeeringDB scopes should be considered minimum for production?

---

Update: 2026-02-10

Decision
1. ZeroTier provisioning will use a provider abstraction with two supported modes:
   - `central` (ZeroTier Central API)
   - `self_hosted_controller` (self-hosted ZeroTier controller API via local service auth)
2. Scope remains control-plane independence only; automated roots/planet management is out of scope for phase 1.

Completed
1. Updated planning docs to remove Central-only assumptions and define provider-based provisioning:
   - `PRD.md`
   - `APP_FLOW.md`
   - `TECH_STACK.md`
   - `BACKEND_STRUCTURE.md`
   - `IMPLEMENTATION_PLAN.md`

In progress
1. No implementation code yet. Documentation alignment is complete.

Next
1. Implement Phase 5 Step 5.1 through Step 5.4:
   - provider interface
   - `central` adapter
   - `self_hosted_controller` adapter
   - Celery task provider selection from `ZT_PROVIDER`
2. Add provider contract tests and adapter selection tests.
3. Update `.env.example` for provider mode and credentials.

Known risks / notes
1. Self-hosted controller endpoint/auth behavior must be validated in integration tests before production rollout.
2. Provider API differences can cause behavior drift without strict contract tests.

---

Update: 2026-02-10 (Phase 1 bootstrap complete)

Completed
1. Implemented Phase 1 Step 1.1 project structure bootstrap:
   - Added `app/`, `tests/`, and `alembic/` scaffolding.
   - Added runnable FastAPI app entrypoint with health endpoint.
2. Implemented Phase 1 Step 1.2 pinned dependency setup:
   - Updated `pyproject.toml` with exact runtime and dev versions from `TECH_STACK.md`.
   - Synced environment with `uv sync --dev` and generated `uv.lock`.
3. Implemented Phase 1 Step 1.3 quality tooling configuration:
   - Added `ruff`, `mypy`, and `pytest` configuration in `pyproject.toml`.
   - Added baseline API health test.
4. Implemented Phase 1 Step 1.4 environment bootstrap:
   - Added `.env.example` with required secrets, endpoint settings, and provider mode variables.
5. Added missing `lessons.md` stub as required by `AGENTS.md`.
6. Updated bootstrap run instructions in `README.md`.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 2 Step 2.1: SQLAlchemy model layer per `BACKEND_STRUCTURE.md`.
2. Start Phase 2 Step 2.2: Alembic initial migration from modeled schema.
3. Add Phase 2 database invariant tests (unique active request constraint and status transition rules).

Known bugs / regressions
1. None identified in Phase 1 bootstrap scope.

Notes
1. `uv run` and `uv sync` required elevated permissions in this environment due cache directory restrictions.

---

Update: 2026-02-10 (Local dependency profile decision documented)

Decision
1. Selected for local development dependencies:
   - Docker Compose for PostgreSQL and Redis only.
   - API/worker/test processes run directly with `uv run`.

Completed
1. Documented the selected local dependency profile in:
   - `TECH_STACK.md`
   - `IMPLEMENTATION_PLAN.md`
   - `README.md`

In progress
1. No implementation code changes in this update.

Next
1. Start Phase 2 Step 2.1: SQLAlchemy model layer per `BACKEND_STRUCTURE.md`.
2. Start Phase 2 Step 2.2: Alembic initial migration from modeled schema.
3. Add Phase 2 database invariant tests (unique active request constraint and status transition rules).

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 2 data and migration foundation complete)

Completed
1. Implemented Phase 2 Step 2.1 SQLAlchemy data model layer per `BACKEND_STRUCTURE.md`:
   - Added DB base/metadata conventions and status enums.
   - Added ORM models for `app_user`, `user_asn`, `zt_network`, `join_request`, `zt_membership`, `oauth_state_nonce`, and `audit_event`.
   - Added DB session helpers.
2. Implemented Phase 2 Step 2.2 initial Alembic migration:
   - Wired Alembic to SQLAlchemy metadata.
   - Added initial revision creating enum, tables, indexes, and active-request partial unique index.
3. Implemented Phase 2 Step 2.3 repository layer:
   - Added repositories for users, user ASN mappings, join requests, memberships, and audit events.
   - Added repository error types for duplicate active request and invalid state transition.
4. Implemented Phase 2 Step 2.4 DB invariant tests:
   - Added DB fixtures and tests covering:
     - one active request per (`asn`, `zt_network_id`)
     - allowed/forbidden request status transitions

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/db -q` passed.
4. `uv run pytest -q` passed.
5. Migration lifecycle verified with `uv run env DATABASE_URL=sqlite:////tmp/zt_ix_phase2.db alembic upgrade/downgrade/upgrade`.

In progress
1. No active implementation tasks.

Next
1. Start Phase 3 Step 3.1: `/auth/login` state/nonce/PKCE generation.
2. Start Phase 3 Step 3.2: `/auth/callback` state validation and token exchange path.
3. Add auth integration tests for success/failure callback branches.

Known bugs / regressions
1. `alembic upgrade head` against local PostgreSQL could not be validated in this session because PostgreSQL on `localhost:5432` was unavailable.

---

Update: 2026-02-10 (Phase 2 PostgreSQL verification gap closed)

Completed
1. Added concrete local dependency orchestration for documented infrastructure profile:
   - Added `docker-compose.yml` with pinned images for PostgreSQL `16.6` and Redis `7.4.1`.
   - Set PostgreSQL host mapping default to `5433` (`${POSTGRES_HOST_PORT:-5433}:5432`) to avoid local `5432` conflicts.
2. Aligned local default DB connection settings to host port `5433`:
   - `.env.example`
   - `app/db/session.py`
   - `alembic.ini`
3. Added README run instructions for starting local dependencies and documented the `5433` host-port expectation.
4. Fixed a PostgreSQL migration defect in `alembic/versions/20260210_0001_phase2_data_foundation.py`:
   - Removed redundant explicit enum creation causing duplicate `request_status` type creation during `upgrade`.
5. Completed previously blocked PostgreSQL migration verification using alternate host port override in this environment:
   - Ran upgrade/downgrade/upgrade successfully against running local PostgreSQL.

Verification
1. `docker compose ps` (PostgreSQL and Redis healthy).
2. `uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic upgrade head && uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic downgrade base && uv run env DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:55433/zt_ix alembic upgrade head` passed.
3. `uv run ruff check .` passed.
4. `uv run mypy .` passed.
5. `uv run pytest tests/db -q` passed.
6. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 3 Step 3.1: `/auth/login` state/nonce/PKCE generation.
2. Start Phase 3 Step 3.2: `/auth/callback` state validation and token exchange path.
3. Add auth integration tests for success/failure callback branches.

Known bugs / regressions
1. None identified in Phase 2 scope after PostgreSQL verification.

Notes
1. In this environment, host port `5433` was already allocated; verification used `POSTGRES_HOST_PORT=55433` while keeping repo defaults at `5433`.
2. Added an AGENTS.md non-negotiable to keep escalation-sensitive commands on consistent prefixes (for example, `docker compose`, `uv run`) to reduce repeated approvals.

---

Update: 2026-02-10 (Implementation plan updated for route-server Option A + Option B TODO)

Decision
1. Route-server automation is now planned as Option A in active scope:
   - Worker-driven SSH orchestration to remote Ubuntu/Linux route servers.
   - Explicit generated BIRD peer configuration per ASN on every configured route server.
   - ROA/RPKI validation required in the BIRD policy/configuration path for generated peers.
2. Option B (persisted per-route-server state model) is deferred and tracked as a TODO phase after Phase 8.

Completed
1. Updated `IMPLEMENTATION_PLAN.md` to Version 0.3:
   - Added Option A assumptions/requirements.
   - Expanded Phase 5 steps and exit criteria for route-server sync via SSH.
   - Added explicit per-ASN peer generation requirement.
   - Added ROA/RPKI validation requirement.
   - Added post-Phase-8 TODO section for Option B.

In progress
1. No implementation code changes in this update.

Next
1. Align `PRD.md`, `APP_FLOW.md`, and `BACKEND_STRUCTURE.md` with the new route-server Option A scope before implementation begins.

---

Update: 2026-02-10 (Phase 4 request workflow complete)

Completed
1. Implemented Phase 4 Step 4.1 request APIs with ownership and duplicate protections:
   - Added `POST /api/v1/requests`, `GET /api/v1/requests`, `GET /api/v1/requests/{request_id}`.
   - Enforced ASN ownership via `user_asn` mappings and deterministic duplicate conflict responses.
   - Added target-network validation for active `zt_network` IDs before request creation.
2. Implemented Phase 4 Step 4.2 operator workflow pages:
   - Added `/dashboard` and `/requests/{request_id}` routes.
   - Expanded `/onboarding` payload to include eligible ASNs and active ZeroTier networks.
3. Implemented Phase 4 Step 4.3 admin queue/detail and decision APIs:
   - Added `/admin/requests` and `/admin/requests/{request_id}`.
   - Added `POST /api/v1/admin/requests/{request_id}/approve`.
   - Added `POST /api/v1/admin/requests/{request_id}/reject` with reject-reason validation.
   - Added `POST /api/v1/admin/requests/{request_id}/retry` with strict `failed -> approved` guard.
   - Added role-based session guards for operator/admin access checks.
4. Implemented Phase 4 Step 4.4 audit emission for workflow transitions:
   - Added `request.created` and `request.status.changed` audit writes with actor/status metadata.
5. Implemented Phase 4 Step 4.5 workflow integration tests:
   - Added `tests/workflow/test_request_workflow_integration.py`.
   - Added workflow test fixtures in `tests/workflow/conftest.py`.
   - Covered create/duplicate/ownership errors, operator visibility guards, admin approve/reject/retry transitions, and admin authorization failures.
6. Added supporting repository and session updates:
   - Added `app/repositories/zt_networks.py`.
   - Added admin queue filters in `JoinRequestRepository.list_for_admin`.
   - Updated user upsert behavior to preserve existing admin flags during OAuth login.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/workflow -q` passed.
4. `uv run pytest -q` passed.

In progress
1. No active implementation tasks.

Next
1. Start Phase 5 Step 5.1-5.4 provider abstraction/adapters and worker provider selection.
2. Replace Phase 4 provisioning enqueue placeholder with actual Celery task dispatch in Phase 5.
3. Add provisioning contract tests under `tests/provisioning`.

Known bugs / regressions
1. None identified in current Phase 4 scope.

Notes
1. Integration coverage added for Phase 4 API/page workflow paths using in-memory SQLite + OAuth stubbed login flow.
2. Define environment/config contract for route-server SSH fanout (inventory, auth, command policy).
3. Start Phase 3 auth implementation once scope docs are aligned.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Phase 3 auth integration complete in code + automated tests)

Completed
1. Implemented Phase 3 Step 3.1 `/auth/login`:
   - Added OAuth state/nonce/PKCE generation and persistence (`oauth_state_nonce`).
   - Added redirect construction to PeeringDB authorize endpoint with PKCE challenge and nonce.
2. Implemented Phase 3 Step 3.2 `/auth/callback`:
   - Added one-time state consumption and expiry handling.
   - Added token exchange path and nonce validation via `id_token` nonce claim.
   - Added explicit callback failure codes and audit events.
3. Implemented Phase 3 Step 3.3 user + ASN sync from PeeringDB profile:
   - Added PeeringDB client abstraction for token/profile calls and profile normalization.
   - Upserts local `app_user` and replaces `user_asn` mappings based on authorized networks.
4. Implemented Phase 3 Step 3.4 session + logout behavior:
   - Added signed cookie session middleware configuration.
   - Added `/auth/logout` session clear + audit event.
   - Added placeholder guarded `/onboarding` and `/error` routes for auth redirects.
5. Implemented Phase 3 Step 3.5 integration-style auth tests (`tests/auth`):
   - Success callback path.
   - Invalid/missing state path.
   - Token exchange failure path.
   - Nonce mismatch path.
   - Callback replay protection path.
6. Added manual browser integration instructions for Phase 3 in `README.md` (required due no browser in this environment).
7. Expanded `.env.example` with auth/session runtime settings (`APP_ENV`, session cookie flags, OAuth TTL/scopes/timeout).

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest tests/auth -q` passed.
4. `uv run pytest -q` passed.

In progress
1. Phase 3 implementation complete.
2. Real browser-based OIDC checks still pending execution outside this environment (instructions documented in `README.md`).

Next
1. Execute the README browser checklist against a real PeeringDB OAuth app and record outcomes.
2. Start Phase 4 Step 4.1 request creation endpoint with ASN ownership checks.
3. Add API auth helpers for current session user to support Phase 4 route guards.

Known bugs / regressions
1. No automated regressions detected in current local test suite.
2. Nonce verification currently depends on `id_token` presence in token response; browser validation against live PeeringDB remains required.

Notes
1. Added test package `__init__.py` files to keep strict `mypy` module resolution stable with multiple `conftest.py` modules.

---

Update: 2026-02-10 (Phase 3 auth nonce fix + detailed callback error surface)

Decision
1. Enforced OIDC scope requirement by always including `openid` in configured PeeringDB scopes to ensure `id_token` nonce validation can succeed.
2. Expanded callback error redirect contract to include user-visible `detail` codes/messages for faster diagnosis without reading server logs.

Completed
1. Fixed likely `invalid_nonce` root cause path:
   - Updated scope normalization in `app/config.py` to inject `openid` when missing and deduplicate configured scopes.
   - Updated `.env.example` default `PEERINGDB_SCOPES` to `openid profile email networks`.
   - Updated `TECH_STACK.md` documented scope list to include `openid`.
2. Added richer callback error details:
   - Extended `/auth/callback` redirect helper to include `detail` query parameter.
   - Added nonce error detail-code mapping (for example `missing_id_token`, `missing_nonce_claim`, `nonce_mismatch`).
   - Added structured detail propagation for `oauth_error`, state errors, and upstream auth failures.
3. Expanded `/error` response payload:
   - Added user-facing `message` per error code.
   - Added passthrough `detail` field for callback diagnostics.
4. Updated tests:
   - Scope assertion now expects `openid profile email networks`.
   - Added coverage for nonce mismatch detail code.
   - Added coverage for missing `id_token` returning `invalid_nonce` with `missing_id_token` detail and message payload.
   - Added config tests for environment-scope normalization (`openid` injection and deduplication).
5. Updated README Phase 3 instructions to explicitly require `PEERINGDB_SCOPES` including `openid`.

Verification
1. `uv run ruff check .` passed.
2. `uv run mypy .` passed.
3. `uv run pytest -q` passed.

In progress
1. No active implementation tasks in this update.

Next
1. Re-run browser integration login against live PeeringDB with updated `PEERINGDB_SCOPES` and verify callback reaches `/onboarding`.
2. Capture any remaining live-provider callback detail codes from `/error` for follow-up hardening if needed.

Known bugs / regressions
1. No regressions detected in local automated checks for this scope.

---

Update: 2026-02-10 (PeeringDB OAuth registration algorithm lesson documented)

Decision
1. Captured a live-integration lesson that successful PeeringDB OIDC callback in this project also depends on OAuth app registration algorithm set to `RSA with SHA-2 256` (RS256).

Completed
1. Updated Phase 3 manual integration guidance in `README.md`:
   - Added explicit OAuth registration prerequisite to set algorithm `RSA with SHA-2 256` (RS256).
2. Updated `TECH_STACK.md` external API constraints:
   - Added PeeringDB registration algorithm requirement as an integration contract note.
3. Replaced `lessons.md` TODO stub with a concrete lesson entry:
   - Symptom, root cause, prevention rule, and good-vs-bad example for the nonce failure scenario.

In progress
1. No active implementation tasks.

Next
1. Keep this registration prerequisite in onboarding/checklist docs for any future environment setup.
2. If additional live-auth pitfalls appear, append them to `lessons.md` and Phase 3 README checklist.

Known bugs / regressions
1. None identified for this documentation-only update.

---

Update: 2026-02-10 (Auth Option A documentation alignment)

Decision
1. Added authentication planning/documentation for Auth Option A:
   - Local credentials table with canonical `app_user` model.
   - Server CLI account provisioning with admin and associated ASN/network options.
2. Clarified option naming to avoid ambiguity with existing route-server options:
   - `Auth Option A` / `Auth Option B`
   - `Route Server Option A` / `Route Server Option B`

Completed
1. Updated `PRD.md` (Version 0.3):
   - Added Auth Option A scope, user stories, acceptance criteria, security expectations, and DoD coverage.
2. Updated `APP_FLOW.md` (Version 0.3):
   - Added `/auth/local/login` flow and explicit server CLI local-account provisioning flow.
3. Updated `BACKEND_STRUCTURE.md` (Version 0.3):
   - Added canonical-user + local-credential data model (`local_credential`, `user_network_access`) and related auth/env/edge-case contracts.
4. Updated `IMPLEMENTATION_PLAN.md` (Version 0.4):
   - Added detailed Auth Option A implementation steps (schema, auth route, CLI provisioning, tests).
   - Disambiguated auth option naming from route-server options.
5. Updated `TECH_STACK.md` (Version 0.3):
   - Documented local credential handling approach and authentication mode support without adding new external dependencies.

In progress
1. No implementation code changes in this update.

Next
1. Resolve open question: for Auth Option A, should empty associated-network assignment be unrestricted or deny-by-default?
2. Start implementation from `IMPLEMENTATION_PLAN.md` Phase 3 Auth Option A steps:
   - schema migration
   - local credential service/route
   - CLI provisioning command
   - tests

Known bugs / regressions
1. None identified for this documentation-only update.

Notes
1. This update changes planning contracts only; runtime behavior is unchanged until implementation starts.
